{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DL_Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n1JW1XTpLje"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.utils import _pair\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AunSqATfpnrY",
        "outputId": "203a138a-d144-4af5-b530-298324d8ea9d"
      },
      "source": [
        "# check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK_0WYhaktul"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqM0yt0EhfJ0"
      },
      "source": [
        "class Conv2D(nn.Module):\n",
        "  \"\"\"Module for calculating a forward pass, on init module needs number of channels of input and output\"\"\"\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=(3,3), stride=1, padding=1):\n",
        "    super(Conv2D, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "    self.kernel = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size[0], kernel_size[1]))\n",
        "\n",
        "  \n",
        "  def forward(self, input_batch):\n",
        "    \"\"\"Forward pass for kernel, using a reshapment an unrolling 2D convolution\"\"\"\n",
        "    batch, in_channels, height, width = input_batch.size()\n",
        "    out_width = (width - self.kernel_size[1] + self.padding * 2 + 1)//self.stride\n",
        "    out_height = (height - self.kernel_size[0] + self.padding * 2 + 1)//self.stride\n",
        "\n",
        "    X = F.unfold(input_batch, self.kernel_size, dilation=1, padding=self.padding, stride=self.stride)\n",
        "    X = X.transpose(1, 2)\n",
        "      \n",
        "    X = X.matmul(self.kernel.view(self.kernel.size(0), -1).t())\n",
        "    X = X.transpose(1, 2)\n",
        "      \n",
        "    batch_output = F.fold(X, (out_height, out_width), (1, 1))\n",
        "\n",
        "    return batch_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iMvgb5khmRQ"
      },
      "source": [
        "conv = Conv2D(3, 1)\n",
        "input_batch = torch.randn(16, 3, 32, 32)\n",
        "output_batch = conv.forward(input_batch)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bntoURUskyHd"
      },
      "source": [
        "class Conv2DFunc(torch.autograd.Function):\n",
        "  \"\"\"\n",
        "  Custom autograd Functions by subclassing torch.autograd.Function and implementing the forward and backward\n",
        "  passes which operate on Tensors.\n",
        "  \"\"\"\n",
        "  @staticmethod\n",
        "  def forward(ctx, input_batch, kernel, kernel_size=(3,3), stride=1, padding=1):\n",
        "    \"\"\"\n",
        "    Tensor containing the input is received as inout and the output is also tensor multiplied by kernel. \n",
        "    Ctx is a context object, that caches arbitrary objects for use in the backward pass using the ctx.save_for_backward method.\n",
        "    \"\"\"\n",
        "    # store objects for the backward\n",
        "    ctx.kernel_size = _pair(kernel_size)\n",
        "    ctx.padding = _pair(padding)\n",
        "    ctx.stride = _pair(stride)\n",
        "\n",
        "    batch, in_channels, height, width = input_batch.size()\n",
        "    out_width = (width - kernel_size[1] + padding * 2 + 1)//stride\n",
        "    out_height = (height - kernel_size[0] + padding * 2 + 1)//stride\n",
        "    print(\"x\",input_batch.shape)\n",
        "    X = F.unfold(input_batch, kernel_size, dilation=1, padding=padding, stride=stride)\n",
        "\n",
        "    X = X.transpose(1, 2)\n",
        "    print(\"x\",X.shape)\n",
        "    X = X.matmul(kernel.view(kernel.shape[0], -1).t())\n",
        "    \n",
        "    print(\"x\",X.shape)\n",
        "\n",
        "    X = X.transpose(1, 2)\n",
        "    \n",
        "    output_batch = F.fold(X, (out_height, out_width), (1, 1))\n",
        "\n",
        "    ctx.save_for_backward(input_batch, output_batch, kernel)\n",
        "    print(\"output_batch\",output_batch.shape)\n",
        "\n",
        "    return output_batch\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    \"\"\"\n",
        "    In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output\n",
        "    and the gradient of the loss with respect to the input and weights is returned\n",
        "    \"\"\"\n",
        "    # retrieve stored objects\n",
        "    input_batch, output_batch, kernel = ctx.saved_tensors\n",
        "    X = F.unfold(input_batch, ctx.kernel_size, dilation=1, padding=ctx.padding, stride=ctx.stride)\n",
        " \n",
        "    transformed_grad_output = grad_output.view(grad_output.shape[0], grad_output.shape[2]* grad_output.shape[3], grad_output.shape[1])\n",
        "    \n",
        "    kernel_grad = X.matmul(transformed_grad_output)\n",
        "    \n",
        "    input_batch_grad = transformed_grad_output.matmul(kernel.view(kernel.shape[0], -1))\n",
        "    \n",
        "    input_batch_grad = input_batch_grad.transpose(1, 2)\n",
        "\n",
        "    kernel_grad = kernel_grad.view(input_batch.shape[0], kernel.shape[1], kernel.shape[2], kernel.shape[3])\n",
        "    print(input_batch_grad.shape)\n",
        "    input_batch_grad = F.fold(input_batch_grad, (input_batch.shape[2], input_batch.shape[3]), (3,3), padding=ctx.padding)\n",
        "    print(input_batch_grad.shape)\n",
        "    return input_batch_grad, kernel_grad, None, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OAr4npnlx1t",
        "outputId": "40af3100-1c30-4a95-fe5e-60e46d9923dd"
      },
      "source": [
        "input_batch = torch.randn(16, 3, 32, 32)\n",
        "out_channels = 1\n",
        "in_channels = input_batch.shape[1]\n",
        "kernel_size = (3,3)\n",
        "kernel = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size[0], kernel_size[1]))\n",
        "\n",
        "function = Conv2DFunc.apply\n",
        "output = function(input_batch, kernel)\n",
        "loss = (output * 0 + 1).sum()\n",
        "\n",
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x torch.Size([16, 3, 32, 32])\n",
            "x torch.Size([16, 1024, 27])\n",
            "x torch.Size([16, 1024, 1])\n",
            "output_batch torch.Size([16, 1, 32, 32])\n",
            "torch.Size([16, 27, 1024])\n",
            "torch.Size([16, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syX4zIiOmnSI",
        "outputId": "584501eb-fc78-4406-bec7-7c460c21dda7"
      },
      "source": [
        "kernel = torch.randn(27, 1)\n",
        "kernel.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zGoyDVnvQMBh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2WsZb--qx1b"
      },
      "source": [
        "# Question 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q5LjxOGJoh_"
      },
      "source": [
        "def get_train_val_loader_Q7(batch_size, augment = False):\n",
        "    \"\"\"\n",
        "    Returns trainloader and valloader of the MNIST dataset. \n",
        "    In case of data augmentation, only trainloader data is augmented.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    normalize = transforms.Normalize((0.1307,), (0.3081,))\n",
        "\n",
        "    if augment:\n",
        "        transform_train = transforms.Compose([transforms.ToTensor(),\n",
        "                                              normalize,\n",
        "                                              transforms.RandomRotation((-2.0,2.0),fill=(1,)),\n",
        "                                              transforms.RandomAffine(10),\n",
        "                                              transforms.RandomPerspective(.2)])\n",
        "    else:\n",
        "        transform_train = transforms.Compose([transforms.ToTensor(), normalize])\n",
        "    \n",
        "    transform_val = transforms.Compose([transforms.ToTensor(), normalize])\n",
        "\n",
        "    # load the same training data twice. transformations should be applied only to training data, not val data in case of augmentation,\n",
        "    # so we should split the training data twice in the exact same way to leave validation data untransformed\n",
        "    train_data = torchvision.datasets.MNIST('/files', train=True, download=True, transform=transform_train)\n",
        "  \n",
        "    val_data = torchvision.datasets.MNIST('/files', train=True, download=True, transform=transform_val)\n",
        "\n",
        "    \n",
        "    num_train = len(train_data)\n",
        "    indices = list(range(num_train))\n",
        "\n",
        "\n",
        "    # shuffle data and separate training and validation data\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, valid_idx = indices[10000:], indices[:10000]\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    # create data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n",
        "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, sampler=val_sampler, num_workers=2)\n",
        "    \n",
        "    return train_loader, val_loader\n",
        "\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt7XFqYWwFHW"
      },
      "source": [
        "def accuracy(model, dataloader, validate=False, criterion=None):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # calculate outputs by running images through the network\n",
        "            outputs = model(images)\n",
        "            \n",
        "            if validate:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            # the class with the highest energy is what we choose as prediction\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    if validate:\n",
        "        return correct / total, loss.item()\n",
        "    else:\n",
        "        return correct / total\n",
        "\n",
        "\"\"\" Training Loop \"\"\"\n",
        "\n",
        "def training_Q7(model, optimizer, criterion, trainloader, n_epochs, validate=None):\n",
        "    # lists for results\n",
        "    loss_train, loss_val, acc_train, acc_val = [], [], [], []\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        running_loss = []\n",
        "\n",
        "        for i, data in enumerate(trainloader):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + loss\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward\n",
        "            loss.backward()\n",
        "\n",
        "            # update\n",
        "            optimizer.step()\n",
        "\n",
        "        #     # loss\n",
        "        #     running_loss.append(loss.item())\n",
        "\n",
        "        #     if i % 1000 == 0:  \n",
        "        #         #print('[%d, %5d]' %(epoch + 1, i))\n",
        "        #         print('training')\n",
        "\n",
        "        # loss_train.append(np.mean(running_loss))\n",
        "\n",
        "        # acc = accuracy(model, trainloader)\n",
        "        # acc_train.append(acc)\n",
        "        # print('Epoch ' + str(epoch+1) + ', training accuracy = ' + str(round(acc, 4)) + ', training loss = ' + str(round(loss_train[-1], 4)))  \n",
        "\n",
        "        if validate:\n",
        "            acc_validate, loss_validate = accuracy(model, validate, True, criterion)\n",
        "            acc_val.append(acc_validate)\n",
        "            loss_val.append(loss_validate)\n",
        "            print('Validation accuracy = ' + str(round(acc_validate, 4))+', validation loss = ' + str(round(loss_val[-1], 4)) + '\\n')\n",
        "\n",
        "    \n",
        "        \n",
        "    \n",
        "    if validate:\n",
        "        return loss_train, acc_train, loss_val, acc_val\n",
        "    else:\n",
        "        return loss_train, acc_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jQcUzOL7z2j"
      },
      "source": [
        "# Question 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLZ0m8Tc71Lr"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, 1, 1)\n",
        "        self.pool  = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, 1, 1)\n",
        "        self.fc    = nn.Linear(576, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NcDhPeEVPSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af7afe8-3b39-4f22-f039-3705a67653ed"
      },
      "source": [
        "\n",
        "\n",
        "learning_rate = 0.0001\n",
        "model = Net()\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()     \n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "trainloader, valloader = get_train_val_loader_Q7(16)\n",
        "\n",
        "loss_train, acc_train, loss_val, acc_val = training_Q7(model, optimizer, criterion, trainloader, 1, valloader)\n",
        "\n",
        "# Plot the resulting loss over time\n",
        "\n",
        "plt.plot(loss_train, label='Training Loss')\n",
        "plt.plot(loss_val, label='Validation Loss', color=\"green\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend();\n",
        "plt.show();\n",
        "plt.plot(acc_train, label='Training Accuracy')\n",
        "plt.plot(acc_val, label='Validation Accuracy', color=\"green\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend();\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy = 0.9541, validation loss = 0.1519\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg8RLV47F9xm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7dKpnkYhmeG"
      },
      "source": [
        "# Question 9 \n",
        "\n",
        "Data augmentation is applied to the trianing data. The same model is trained on the augmented data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xAAOlgu2q0so",
        "outputId": "ff5bfe8e-8464-417c-dd61-0fb2c41d22f4"
      },
      "source": [
        "\"\"\"\n",
        "Let's train the model on the augmented data\n",
        "\"\"\"\n",
        "# transform train data\n",
        "trainloader_aug, valloader_aug = get_train_val_loader_Q7(16, augment=True)\n",
        "\n",
        "\n",
        "learning_rate = 0.0001\n",
        "\n",
        "\n",
        "model_aug = Net()\n",
        "model_aug.to(device)\n",
        "\n",
        "criterion_aug = nn.CrossEntropyLoss()     \n",
        "optimizer_aug = optim.Adam(model_aug.parameters(), lr = learning_rate)\n",
        "\n",
        "loss_train_aug, acc_train_aug, loss_val_aug, acc_val_aug = training_Q7(model_aug, optimizer_aug, criterion_aug, trainloader_aug, 20, valloader_aug)\n",
        "loss_train, acc_train, loss_val, acc_val = training_Q7(model, optimizer, criterion, trainloader, 20, valloader)\n",
        "\n",
        "print(acc_val_aug)\n",
        "print(acc_val)\n",
        "\n",
        "plt.plot(acc_train, label='non-augmented Accuracy')\n",
        "plt.plot(acc_train_aug, label='Augmented data Accuracy', color=\"green\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend();\n",
        "plt.show();\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy = 0.9589, validation loss = 0.0976\n",
            "\n",
            "Validation accuracy = 0.9718, validation loss = 0.3882\n",
            "\n",
            "Validation accuracy = 0.9773, validation loss = 0.0176\n",
            "\n",
            "Validation accuracy = 0.9797, validation loss = 0.0644\n",
            "\n",
            "Validation accuracy = 0.9813, validation loss = 0.0083\n",
            "\n",
            "Validation accuracy = 0.9831, validation loss = 0.0168\n",
            "\n",
            "Validation accuracy = 0.9874, validation loss = 0.002\n",
            "\n",
            "Validation accuracy = 0.986, validation loss = 0.0037\n",
            "\n",
            "Validation accuracy = 0.9877, validation loss = 0.4111\n",
            "\n",
            "Validation accuracy = 0.9878, validation loss = 0.0007\n",
            "\n",
            "Validation accuracy = 0.9893, validation loss = 0.0756\n",
            "\n",
            "Validation accuracy = 0.9873, validation loss = 0.0001\n",
            "\n",
            "Validation accuracy = 0.9891, validation loss = 0.0099\n",
            "\n",
            "Validation accuracy = 0.9879, validation loss = 0.009\n",
            "\n",
            "Validation accuracy = 0.99, validation loss = 0.0066\n",
            "\n",
            "Validation accuracy = 0.9901, validation loss = 0.0003\n",
            "\n",
            "Validation accuracy = 0.9914, validation loss = 0.0143\n",
            "\n",
            "Validation accuracy = 0.9916, validation loss = 0.1569\n",
            "\n",
            "Validation accuracy = 0.9901, validation loss = 0.1076\n",
            "\n",
            "Validation accuracy = 0.991, validation loss = 0.0012\n",
            "\n",
            "Validation accuracy = 0.9865, validation loss = 0.0441\n",
            "\n",
            "Validation accuracy = 0.985, validation loss = 0.0106\n",
            "\n",
            "Validation accuracy = 0.9869, validation loss = 0.0018\n",
            "\n",
            "Validation accuracy = 0.9863, validation loss = 0.0012\n",
            "\n",
            "Validation accuracy = 0.9873, validation loss = 0.0149\n",
            "\n",
            "Validation accuracy = 0.9882, validation loss = 0.0003\n",
            "\n",
            "Validation accuracy = 0.9889, validation loss = 0.0148\n",
            "\n",
            "Validation accuracy = 0.988, validation loss = 0.0097\n",
            "\n",
            "Validation accuracy = 0.9891, validation loss = 0.0003\n",
            "\n",
            "Validation accuracy = 0.9868, validation loss = 0.0014\n",
            "\n",
            "Validation accuracy = 0.9878, validation loss = 0.0095\n",
            "\n",
            "Validation accuracy = 0.9871, validation loss = 0.1906\n",
            "\n",
            "Validation accuracy = 0.9878, validation loss = 0.0008\n",
            "\n",
            "Validation accuracy = 0.9861, validation loss = 0.4933\n",
            "\n",
            "Validation accuracy = 0.9894, validation loss = 0.0001\n",
            "\n",
            "Validation accuracy = 0.9887, validation loss = 0.0013\n",
            "\n",
            "Validation accuracy = 0.9892, validation loss = 0.0033\n",
            "\n",
            "Validation accuracy = 0.9894, validation loss = 0.0003\n",
            "\n",
            "Validation accuracy = 0.989, validation loss = 0.0002\n",
            "\n",
            "Validation accuracy = 0.9869, validation loss = 0.0888\n",
            "\n",
            "[0.9589, 0.9718, 0.9773, 0.9797, 0.9813, 0.9831, 0.9874, 0.986, 0.9877, 0.9878, 0.9893, 0.9873, 0.9891, 0.9879, 0.99, 0.9901, 0.9914, 0.9916, 0.9901, 0.991]\n",
            "[0.9865, 0.985, 0.9869, 0.9863, 0.9873, 0.9882, 0.9889, 0.988, 0.9891, 0.9868, 0.9878, 0.9871, 0.9878, 0.9861, 0.9894, 0.9887, 0.9892, 0.9894, 0.989, 0.9869]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], acc_val, label='non-augmented Accuracy')\n",
        "plt.plot([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], acc_val_aug, label='Augmented Accuracy', color=\"green\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend();\n",
        "plt.show();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "awAr6hciZQxl",
        "outputId": "0f72e218-b59b-4f25-931a-97bca2589a43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRfz3cTuquir",
        "outputId": "6029c595-c348-404b-db22-bd8202e4f0ae"
      },
      "source": [
        "\"\"\"\n",
        "Reporting accuracies on test data for model trained on normal data and model trained on augmented\n",
        "\"\"\"\n",
        "# get test data and test_loader\n",
        "normalize = transforms.Normalize((0.1307,), (0.3081,))\n",
        "\n",
        "test_data = torchvision.datasets.MNIST('/files', train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), normalize]))\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=100, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "# accuracy for non-augmented data:\n",
        "test_accuracy = accuracy(model, testloader)\n",
        "print('For non-augmented data, final accuracy is ' + str(round(test_accuracy, 4)))\n",
        "\n",
        "# accuracy for augmented data:\n",
        "test_accuracy_aug = accuracy(model_aug, testloader)\n",
        "print('For augmented data, final accuracy is ' + str(round(test_accuracy_aug, 4)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For non-augmented data, final accuracy is 0.9878\n",
            "For augmented data, final accuracy is 0.9767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PM24WErhCeX"
      },
      "source": [
        "# Question 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkDW92Hhr0fi"
      },
      "source": [
        "# Question 11 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll77yKv9hEWA",
        "outputId": "e705dc71-0da6-4aa3-eccf-aa154d755020"
      },
      "source": [
        "# just examples\n",
        "b, c, h, w = 8, 3, 1024, 768\n",
        "\n",
        "# input tensor\n",
        "x = torch.randn(b, c, h, w)\n",
        "\n",
        "# implementing global max and mean with just one line per operation\n",
        "def global_max(x): return nn.MaxPool2d(x.size()[2:])(x).squeeze()\n",
        "def global_mean(x): return nn.AvgPool2d(x.size()[2:])(x).squeeze()\n",
        "\n",
        "y_max = global_max(x)\n",
        "y_mean = global_mean(x)\n",
        "\n",
        "# check if size is as desired!\n",
        "print(y_max.size())\n",
        "print(y_mean.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 3])\n",
            "torch.Size([8, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaVirtlI3wOR"
      },
      "source": [
        "# Question 12 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OyX5xig3vr7"
      },
      "source": [
        "# Downloading and extracting the images\n",
        "! wget https://dlvu.github.io/data/mnist-varres.tgz\n",
        "!tar -xvf 'mnist-varres.tgz' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oWp4kmp57vB"
      },
      "source": [
        "normalize = transforms.Normalize((0.1307,), (0.3081,))\n",
        "\n",
        "# resizing the images and making them have 1 channel rather than 3\n",
        "transform_train = transforms.Compose([transforms.ToTensor(), normalize, transforms.Resize([28, 28]), transforms.Grayscale(1)]) \n",
        "\n",
        "root = 'mnist-varres/train'\n",
        "train_v = torchvision.datasets.ImageFolder(root, transform = transform_train)\n",
        "\n",
        "trainset, valset = torch.utils.data.random_split(train_v, [55000, 5000])\n",
        "\n",
        "# Creating dataloaders \n",
        "train_loader_28 = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader_28 = torch.utils.data.DataLoader(valset, batch_size=16, shuffle=True, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlNXT0Uo6Tec",
        "outputId": "1e5ee500-6b88-498d-ff00-07c1ecc20097"
      },
      "source": [
        "learning_rate = 0.005\n",
        "model_Q12 = Net()\n",
        "model_Q12.to(device)\n",
        "\n",
        "criterion_Q12 = nn.CrossEntropyLoss()     \n",
        "optimizer_Q12 = optim.Adam(model_Q12.parameters(), lr = learning_rate)\n",
        "\n",
        "loss_train_Q12, acc_train_Q12, loss_val_Q12, acc_val_Q12 = training_Q7(model_Q12, optimizer_Q12, criterion_Q12, train_loader_28, 2, val_loader_28)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training\n",
            "training\n",
            "training\n",
            "training\n",
            "Epoch 1, training accuracy = 0.9348, training loss = 0.4193\n",
            "Validation accuracy = 0.9248, validation loss = 0.2414\n",
            "\n",
            "training\n",
            "training\n",
            "training\n",
            "training\n",
            "Epoch 2, training accuracy = 0.9389, training loss = 0.2271\n",
            "Validation accuracy = 0.9324, validation loss = 0.1635\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0wNlgl9CCEB"
      },
      "source": [
        "# Question 14\n",
        "\n",
        "First the data is sorted based on size. The sorted data is placed in new folders on the drive. After this a custom Dataset class is constructed to get the data from the new folders. Next, a function is defined to create dataloaders from the custom datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcqeSkHqbpqP"
      },
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def make_folders(instance = 'train'):\n",
        "    \"\"\"\n",
        "    Create new folders for the MNIST images. Folders are separated by size, x and y, and training or test instance\n",
        "    \"\"\"\n",
        "    root = 'mnist-varres/' + instance + '/'\n",
        "    l = list(range(10))\n",
        "\n",
        "    data_32, data_48, data_64 = [[],[]], [[],[]], [[],[]]\n",
        "\n",
        "    for label in l:\n",
        "        images = glob.glob(root+str(label)+ '/*.png')\n",
        "\n",
        "        for image in images:\n",
        "            image = np.asarray(Image.open(image))\n",
        "            if np.shape(image)[0] == 32:\n",
        "                data_32[0].append([image])\n",
        "                data_32[1].append(label)\n",
        "\n",
        "            elif np.shape(image)[0] == 48:\n",
        "                data_48[0].append([image])\n",
        "                data_48[1].append(label)\n",
        "                \n",
        "            elif np.shape(image)[0] == 64:\n",
        "                data_64[0].append([image])\n",
        "                data_64[1].append(label)\n",
        "        \n",
        "\n",
        "    data_32_x, data_32_y = np.asarray(data_32[0]), np.asarray(data_32[1])\n",
        "    data_48_x, data_48_y = np.asarray(data_48[0]), np.asarray(data_48[1])\n",
        "    data_64_x, data_64_y = np.asarray(data_64[0]), np.asarray(data_64[1])\n",
        "\n",
        "    np.save('/content/32_x_' + instance, data_32_x, allow_pickle = True)\n",
        "    np.save('/content/48_x_' + instance, data_48_x, allow_pickle = True)\n",
        "    np.save('/content/64_x_' + instance, data_64_x, allow_pickle = True)\n",
        "\n",
        "    np.save('/content/32_y_' + instance, data_32_y, allow_pickle = True)\n",
        "    np.save('/content/48_y_' + instance, data_48_y, allow_pickle = True)\n",
        "    np.save('/content/64_y_' + instance, data_64_y, allow_pickle = True)\n",
        "\n",
        "\n",
        "make_folders()\n",
        "make_folders('test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uQCrKd7b-NC"
      },
      "source": [
        "class VarDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size = 32, instance = 'train'):\n",
        "        file_x = '/content/' + str(size) + '_x_' + instance + '.npy'\n",
        "        file_y = '/content/' + str(size) + '_y_' + instance + '.npy'\n",
        "        x = np.load(file_x, allow_pickle = True)\n",
        "        y = np.load(file_y, allow_pickle = True)\n",
        "        \n",
        "        x = x.astype('float32')\n",
        "\n",
        "        self.n_samples = np.shape(x)[0]\n",
        "\n",
        "        self.x_data = torch.from_numpy(x) \n",
        "        self.y_data = torch.from_numpy(y) \n",
        "        self.y_data = self.y_data.to(torch.long)\n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "data_32_train_full = VarDataset(32)\n",
        "data_48_train_full = VarDataset(48)\n",
        "data_64_train_full = VarDataset(64)\n",
        "\n",
        "data_32_test = VarDataset(32, 'test')\n",
        "data_48_test = VarDataset(48, 'test')\n",
        "data_64_test = VarDataset(64, 'test')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQXfxwtyrXUE"
      },
      "source": [
        "import math\n",
        "\n",
        "def get_data_loaders_Q14(dataset, batch_size):\n",
        "    split = math.floor(len(dataset)/10) * 8\n",
        "\n",
        "    trainset, valset = torch.utils.data.random_split(dataset, [split, len(dataset)-split])\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "    valloader = torch.utils.data.DataLoader(valset, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "\n",
        "    return trainloader, valloader\n",
        "\n",
        "data_32_train, data_32_val = get_data_loaders_Q14(data_32_train_full, 16)\n",
        "data_48_train, data_48_val = get_data_loaders_Q14(data_48_train_full, 16)\n",
        "data_64_train, data_64_val = get_data_loaders_Q14(data_64_train_full, 16)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YS6N_EK5z9u"
      },
      "source": [
        "def training_Q14(model, optimizer, criterion, trainloaders, n_epochs, valloaders):\n",
        "    # lists for results\n",
        "    loss_train, acc_train, loss_val, acc_val = [], [], [], []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        running_loss = []\n",
        "        total_train = 0\n",
        "        correct_train = 0\n",
        "\n",
        "        # go through list of trainloaders\n",
        "        for trainloader in trainloaders:\n",
        "\n",
        "            for i, data in enumerate(trainloader):\n",
        "                inputs, labels = data[0].to(device), data[1].to(device)\n",
        "                total_train += labels.size(0)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward + loss\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    _, predicted = torch.max(outputs.data, -1)\n",
        "                    correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "                # backward\n",
        "                loss.backward()\n",
        "\n",
        "                # update\n",
        "                optimizer.step()\n",
        "\n",
        "                # loss\n",
        "                running_loss.append(loss.item())\n",
        "                if i % 500 == 0:\n",
        "                    print('Training.....')\n",
        "\n",
        "            \n",
        "\n",
        "        loss_train.append(np.mean(running_loss))\n",
        "        acc_train.append(correct_train/ total_train)\n",
        "\n",
        "        print('Epoch ' + str(epoch+1) + ', training accuracy = ' + str(round(acc_train[-1], 4)) + ', training loss = ' + str(round(loss_train[-1], 4)))  \n",
        "        \n",
        "\n",
        "        \"\"\" Get validation loss and accuracy \"\"\"\n",
        "        total_val_correct = 0\n",
        "        total_val = 0\n",
        "        running_val_loss = []\n",
        "        for valloader in valloaders:\n",
        "            for data in valloader:\n",
        "                inputs, labels = data[0].to(device), data[1].to(device)\n",
        "                total_val += labels.size(0)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(inputs)\n",
        "                    labels = labels.to(torch.long)\n",
        "                    #loss = criterion(outputs, labels)\n",
        "                    _, predicted = torch.max(outputs.data, -1)\n",
        "                    total_val_correct += (predicted == labels).sum().item()\n",
        "                    #running_val_loss.append(loss.item())\n",
        "\n",
        "\n",
        "        #loss_val.append(np.mean(running_val_loss))    \n",
        "        acc_val.append(total_val_correct/ total_val)\n",
        "        #print('Validation accuracy = ' + str(round(acc_val[-1], 4))+', validation loss = ' + str(round(loss_val[-1], 4)) + '\\n')\n",
        "        print('Validation accuracy = ' + str(round(acc_val[-1], 4))+'\\n')\n",
        "\n",
        "    return loss_train, acc_train, loss_val, acc_val\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-MQCoC1_2nx"
      },
      "source": [
        "# Question 15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbHcoXLDADzH",
        "outputId": "87ffad6f-da44-41db-e572-d39be60613d1"
      },
      "source": [
        "\"\"\" Count parameters of the CNN of the previous section \"\"\"\n",
        "model = Net()\n",
        "\n",
        "def get_n_params(model):\n",
        "    params = 0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        params += nn\n",
        "    return params\n",
        "\n",
        "get_n_params(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29066"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PS10OkfTl98",
        "outputId": "55476fe6-26f4-4171-9e68-4051fdbbc05d"
      },
      "source": [
        "def count_params(N):\n",
        "    \"\"\"\n",
        "    Function that calculates the number of parameters for the new CNN model given N\n",
        "    \"\"\"\n",
        "    return 16*10 + 32*(1+16*9) + N*(1+32*9) + (1+N)*10\n",
        "\n",
        "# Trial-and-error shows that N=81 is the best choice\n",
        "print(count_params(80))\n",
        "print(count_params(81))\n",
        "print(count_params(82))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28730\n",
            "29029\n",
            "29328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8dOgl44_5Lu"
      },
      "source": [
        "\"\"\" Defining the new CNN \"\"\"\n",
        "\n",
        "class Net_N(nn.Module):\n",
        "    def __init__(self, N, pool_type='max'):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, 1, 1)\n",
        "        self.pool  = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\n",
        "        self.conv3 = nn.Conv2d(32, N, 3, 1, 1)\n",
        "        self.lin = nn.Linear(N, 10)\n",
        "        self.pool_type = pool_type\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        if self.pool_type == 'max':\n",
        "            x = self.lin(nn.MaxPool2d(x.size()[2:])(x).squeeze())\n",
        "        elif self.pool_type == 'mean':\n",
        "            x = self.lin(nn.AvgPool2d(x.size()[2:])(x).squeeze())\n",
        "\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCGc3loFGc8k"
      },
      "source": [
        "# Question 16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfefN5t397hy",
        "outputId": "81cb6134-2536-456d-cddd-8ad226733bf8"
      },
      "source": [
        "# create CNN (global max)\n",
        "model_max = Net_N(81)\n",
        "model_max.to(device)\n",
        "\n",
        "learning_rate = 0.001\n",
        "criterion_max = nn.CrossEntropyLoss()     \n",
        "optimizer_max = optim.Adam(model_max.parameters(), lr = learning_rate)\n",
        "\n",
        "# create lists of the dataloaders with different image sizes\n",
        "trainloaders = [data_32_train, data_48_train, data_64_train]\n",
        "valloaders = [data_32_val, data_48_val, data_64_val]\n",
        "\n",
        "\n",
        "# train the model\n",
        "loss_train, acc_train, loss_val, acc_val = training_Q14(model_max, optimizer_max, criterion_max, trainloaders, 3, valloaders)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Epoch 1, training accuracy = 0.9157, training loss = 0.2965\n",
            "Validation accuracy = 0.9568\n",
            "\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Epoch 2, training accuracy = 0.9704, training loss = 0.0996\n",
            "Validation accuracy = 0.9749\n",
            "\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Epoch 3, training accuracy = 0.9762, training loss = 0.0772\n",
            "Validation accuracy = 0.9664\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQtRpJBiG1b3",
        "outputId": "a508f4a6-8acc-47ac-b044-dc4dea19bb2f"
      },
      "source": [
        "\"\"\" Now comparing to a CNN with a global mean! \"\"\"\n",
        "# create CNN (global man)\n",
        "model_mean = Net_N(81, pool_type='mean')\n",
        "model_mean.to(device)\n",
        "\n",
        "learning_rate = 0.001\n",
        "criterion_mean = nn.CrossEntropyLoss()     \n",
        "optimizer_mean = optim.Adam(model_mean.parameters(), lr = learning_rate)\n",
        "\n",
        "# create lists of the dataloaders with different image sizes\n",
        "trainloaders = [data_32_train, data_48_train, data_64_train]\n",
        "valloaders = [data_32_val, data_48_val, data_64_val]\n",
        "\n",
        "\n",
        "# train the model\n",
        "loss_train, acc_train, loss_val, acc_val = training_Q14(model_mean, optimizer_mean, criterion_mean, trainloaders, 3, valloaders)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Epoch 1, training accuracy = 0.9151, training loss = 0.2867\n",
            "Validation accuracy = 0.9638\n",
            "\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Epoch 2, training accuracy = 0.9749, training loss = 0.0927\n",
            "Validation accuracy = 0.9823\n",
            "\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Training.....\n",
            "Epoch 3, training accuracy = 0.9813, training loss = 0.069\n",
            "Validation accuracy = 0.981\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV_rlS9IHed6"
      },
      "source": [
        "Looks like a global mean performs better! Next we'll compare it to the original CNN network where we resize all images to shape 64x64. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 17\n"
      ],
      "metadata": {
        "id": "f4FvWUPEdk1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(model, dataloader, validate=False, criterion=None):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # calculate outputs by running images through the network\n",
        "            outputs = model(images)\n",
        "            \n",
        "            if validate:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            # the class with the highest energy is what we choose as prediction\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    if validate:\n",
        "        return correct / total, loss.item()\n",
        "    else:\n",
        "        return correct / total\n",
        "\n",
        "\"\"\" Training Loop \"\"\"\n",
        "\n",
        "def training_Q17(model, optimizer, criterion, trainloader, n_epochs, validate=None):\n",
        "    # lists for results\n",
        "    loss_train, loss_val, acc_train, acc_val = [], [], [], []\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        running_loss = []\n",
        "\n",
        "        for i, data in enumerate(trainloader):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + loss\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backward\n",
        "            loss.backward()\n",
        "\n",
        "            # update\n",
        "            optimizer.step()\n",
        "\n",
        "            # loss\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "            if i % 1000 == 0:  \n",
        "                #print('[%d, %5d]' %(epoch + 1, i))\n",
        "                print('training')\n",
        "\n",
        "        loss_train.append(np.mean(running_loss))\n",
        "\n",
        "        acc = accuracy(model, trainloader)\n",
        "        acc_train.append(acc)\n",
        "        print('Epoch ' + str(epoch+1) + ', training accuracy = ' + str(round(acc, 4)) + ', training loss = ' + str(round(loss_train[-1], 4)))  \n",
        "\n",
        "        if validate:\n",
        "            acc_validate, loss_validate = accuracy(model, validate, True, criterion)\n",
        "            acc_val.append(acc_validate)\n",
        "            loss_val.append(loss_validate)\n",
        "            print('Validation accuracy = ' + str(round(acc_validate, 4))+', validation loss = ' + str(round(loss_val[-1], 4)) + '\\n')\n",
        "\n",
        "    \n",
        "        \n",
        "    \n",
        "    if validate:\n",
        "        return loss_train, acc_train, loss_val, acc_val\n",
        "    else:\n",
        "        return loss_train, acc_train"
      ],
      "metadata": {
        "id": "KDPy1TLmdo_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" CNN with input all resized to 28x28 \"\"\" \n",
        "\n",
        "learning_rate = 0.001\n",
        "model_resized = Net()\n",
        "model_resized.to(device)\n",
        "\n",
        "criterion_resized = nn.CrossEntropyLoss()     \n",
        "optimizer_resized = optim.Adam(model_resized.parameters(), lr = learning_rate)\n",
        "\n",
        "loss_train, acc_train, loss_val, acc_val = training_Q17(model_resized, optimizer_resized, criterion_resized, train_loader_28, 5, val_loader_28)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZJoGaolrdts2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" CNN with varied resolution input \"\"\"\n",
        "\n",
        "learning_rate = 0.001\n",
        "model_var_N = Net_N(81, 'mean')\n",
        "model_var_N.to(device)\n",
        "\n",
        "criterion_var_N = nn.CrossEntropyLoss()     \n",
        "optimizer_var_N = optim.Adam(model_var_N.parameters(), lr = learning_rate)\n",
        "\n",
        "trainloaders = [data_32_train, data_48_train, data_64_train]\n",
        "valloaders = [data_32_val, data_48_val, data_64_val]\n",
        "\n",
        "\n",
        "loss_train, acc_train, loss_val, acc_val = training_Q14(model_var_N, optimizer_var_N, criterion_var_N, trainloaders, 5, valloaders)\n",
        "\n"
      ],
      "metadata": {
        "id": "IIHL6cUzdvQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "Deeper CNN with varied resolution input \n",
        "This one has an extra convolution layer and an extra maxpooling layer.\n",
        "\n",
        "\"\"\"\n",
        " \n",
        "\n",
        "class Net_M(nn.Module):\n",
        "    def __init__(self, N, M, pool_type='max'):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, 1, 1)\n",
        "        self.pool  = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\n",
        "        self.conv3 = nn.Conv2d(32, N, 3, 1, 1)\n",
        "        self.conv4 = nn.Conv2d(N, M, 3, 1, 1)\n",
        "        self.lin = nn.Linear(M, 10)\n",
        "        self.pool_type = pool_type\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        if self.pool_type == 'max':\n",
        "            x = self.lin(nn.MaxPool2d(x.size()[2:])(x).squeeze())\n",
        "        elif self.pool_type == 'mean':\n",
        "            x = self.lin(nn.AvgPool2d(x.size()[2:])(x).squeeze())\n",
        "\n",
        "        return x\n",
        "\n",
        "\"\"\"\n",
        "N and M are chosen such that the network has rougly the same number of weights as before. \n",
        "\"\"\"\n",
        "N = 46\n",
        "M = 26\n",
        "\n",
        "def calc(N, M):\n",
        "    count = 16*10 + 32*(16*3*3+1) + N*(32*3*3+1) + M*(N*3*3+1) + 10*(M+1)\n",
        "    return count\n",
        "\n",
        "print(calc(N, M))"
      ],
      "metadata": {
        "id": "oB4gMu2FdwzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "model_var = Net_M(46, 26, 'mean')\n",
        "model_var.to(device)\n",
        "\n",
        "criterion_var = nn.CrossEntropyLoss()     \n",
        "optimizer_var = optim.Adam(model_var.parameters(), lr = learning_rate)\n",
        "\n",
        "trainloaders = [data_32_train, data_48_train, data_64_train]\n",
        "valloaders = [data_32_val, data_48_val, data_64_val]\n",
        "\n",
        "\n",
        "loss_train, acc_train, loss_val, acc_val = training_Q14(model_var, optimizer_var, criterion_var, trainloaders, 5, valloaders)\n",
        "\n"
      ],
      "metadata": {
        "id": "SAT77hmad02h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Getting test data to resize everything to 28x28 \"\"\" \n",
        "\n",
        "normalize = transforms.Normalize((0.1307,), (0.3081,))\n",
        "\n",
        "# resizing the images and making them have 1 channel rather than 3\n",
        "transform_test = transforms.Compose([transforms.ToTensor(), normalize, transforms.Resize([28, 28]), transforms.Grayscale(1)]) \n",
        "\n",
        "root = 'mnist-varres/test'\n",
        "testset = torchvision.datasets.ImageFolder(root, transform = transform_test)\n",
        "\n",
        "# Creating dataloader\n",
        "testloader_28 = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "\"\"\" Getting test data separated in 3 folders based on resolution \"\"\"\n",
        "\n",
        "# creating datasets\n",
        "data_32_test = VarDataset(32, 'test')\n",
        "data_48_test = VarDataset(48, 'test')\n",
        "data_64_test = VarDataset(64, 'test')\n",
        "\n",
        "# creating dataloaders\n",
        "testloader_32 = torch.utils.data.DataLoader(data_32_test, batch_size =32, shuffle=True, num_workers=2)\n",
        "testloader_48 = torch.utils.data.DataLoader(data_48_test, batch_size =32, shuffle=True, num_workers=2)\n",
        "testloader_64 = torch.utils.data.DataLoader(data_64_test, batch_size =32, shuffle=True, num_workers=2)\n"
      ],
      "metadata": {
        "id": "I28Pgrk0d2y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Testing the models \"\"\" \n",
        "\n",
        "def test_accuracy_Q17(model, testloaders):\n",
        "    \"\"\"\n",
        "    Second parameter requires a list of testloaders (may contain just one testloader)\n",
        "    \"\"\"\n",
        "\n",
        "    total = 0 \n",
        "    correct = 0 \n",
        "\n",
        "    for testloader in testloaders:\n",
        "        for i, data in enumerate(testloader):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            \n",
        "            total += labels.size(0)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, -1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    print('Accuracy on test set = ' + str(round(correct/total, 4)))\n",
        "\n",
        "\n",
        "# testing resized input CNN\n",
        "print('For the network with input resized to 28x28: ')\n",
        "test_accuracy_Q17(model_resized, [testloader_28])\n",
        "\n",
        "testloaders_var = [testloader_32, testloader_48, testloader_64]\n",
        "\n",
        "print('\\nFor the network with varied input (Q16):')\n",
        "test_accuracy_Q17(model_var_N, testloaders_var)\n",
        "\n",
        "print('\\nFor the deeper network with varied input (Q17):')\n",
        "test_accuracy_Q17(model_var, testloaders_var)"
      ],
      "metadata": {
        "id": "MhbxIPg3d4Fl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}