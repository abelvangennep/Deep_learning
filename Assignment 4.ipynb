{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ_pmgxvGur9"
      },
      "source": [
        "# Assignment 4 - Graph Convolutional Networks\n",
        "## Deep Learning Course - Vrije Universiteit Amsterdam, 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEneMITS2agU"
      },
      "source": [
        "#### Instructions on how to use this notebook:\n",
        "\n",
        "This notebook is hosted on Google Colab. To be able to work on it, you have to create your own copy. Go to *File* and select *Save a copy in Drive*.\n",
        "\n",
        "You can also avoid using Colab entirely, and download the notebook to run it on your own machine. If you choose this, go to *File* and select *Download .ipynb*.\n",
        "\n",
        "The advantage of using Colab is that you can use a GPU. You can complete this assignment with a CPU, but it will take a bit longer. Furthermore, we encourage you to train using the GPU not only for faster training, but also to get experience with this setting. This includes moving models and tensors to the GPU and back. This experience is vary valuable because for many interesting models and large datasets (like large CNNs for ImageNet, or Transformer models trained on Wikipedia), training on GPU is the only feasible way.\n",
        "\n",
        "The default Colab runtime does not have a GPU. To change this, go to *Runtime - Change runtime type*, and select *GPU* as the hardware accelerator. The GPU that you get changes according to what resources are available at the time, and its memory can go from a 5GB, to around 18GB if you are lucky. If you are curious, you can run the following in a code cell to check:\n",
        "\n",
        "```sh\n",
        "!nvidia-smi\n",
        "```\n",
        "\n",
        "Note that despite the name, Google Colab does  not support collaborative work without issues. When two or more people edit the notebook concurrently, only one version will be saved. You can choose to do group programming with one person sharing the screen with the others, or make multiple copies of the notebook to work concurrently.\n",
        "\n",
        "**Submission:** Upload your notebook in .ipynb format to Canvas. The code and answers to the questions in the notebook are sufficient, no separate report is expected. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBgoJIpdLI2Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b950404-32c3-4634-9058-ad1c9b68112f"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec 19 20:33:37 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P8    75W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsdc7fDp40rQ"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "Graphs are very useful data structures that allow us to represent sets of entities and the way they are related among each other. In a graph, entities are also known as *nodes*, and any link between entities is also called an *edge*.\n",
        "\n",
        "Examples of real world objects that can be modeled as graphs are social networks, where entities are people and relations denote friendship; and molecules, where entities are atoms and relations indicate a bond between them.\n",
        "\n",
        "There has been increased interest in the recent years in the application of deep learning architectures to graph-structured data, for tasks like predicting missing relations between entities, classifying entities, and classifying graphs. This interest has been spurred by the introduction of Graph Convolutional Networks (GCNs).\n",
        "\n",
        "In this assignment, you will implement and experiment with one of the first versions of the GCN, proposed by Thomas Kipf and Max Welling in their 2017 paper, [Semi-supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907). In particular, the goals of this assignment are to\n",
        "\n",
        "- Understand how GCNs are formulated\n",
        "- Implement the GCN using PyTorch\n",
        "- Train and evaluate a model for semi-supervised node classification in citation networks\n",
        "- Train and evaluate a model for binary classification of molecules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvsuVNczG6pP"
      },
      "source": [
        "### Representing graphs\n",
        "\n",
        "Suppose we have the following graph:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/01-graph.png\" width=\"200\">\n",
        "\n",
        "This is an undirected graph (since the edges have no specified direction) with 4 nodes. One way to represent the connectivity structure of the graph is by means of the **adjacency matrix**. The $i$-th row of the matrix contains a 1 in the $j$-th column, if nodes $i$ and $j$ are connected. For an undirected graph like the one above, this means that the adjacency matrix\n",
        "\n",
        "- Is symmetric (e.g. an edge between 0 and 2 is equivalent as an edge between 2 and 0)\n",
        "- Is square, of size $n\\times n$ where $n$ is the number of nodes\n",
        "\n",
        "The adjacency matrix for the graph above is then the following:\n",
        "\n",
        "$$\n",
        "A =\n",
        "\\begin{bmatrix}\n",
        "0 & 0 & 1 & 0 \\\\ \n",
        "0 & 0 & 1 & 0 \\\\\n",
        "1 & 1 & 0 & 1 \\\\\n",
        "0 & 0 & 1 & 0\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "A second matrix of interest is the **degree matrix**. This is a diagonal matrix where the $i$-th element of the diagonal indicates the number of edges connected to node $i$. Note that these can be obtained from $A$ by summing across the columns, or the rows. For our example, the degree matrix is\n",
        "\n",
        "$$\n",
        "D = \\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 \\\\ \n",
        "0 & 1 & 0 & 0 \\\\\n",
        "0 & 0 & 3 & 0 \\\\\n",
        "0 & 0 & 0 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "For specific applications, each node in the graph will have an associated vector of features $x\\in\\mathbb{R}^c$. If our graph is a social network, then the vector of features can contain information like age, location, and musical tastes, in a specific numeric format. In the case of a molecule, the node could represent an atom and have features like the atomic mass, etc. We can lay out the features in a matrix $X\\in\\mathbb{R}^{n\\times c}$, so that the feature vector for node $i$ is in the $i$-th row."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCEQ2ffzHCf2"
      },
      "source": [
        "### Loading a citation network\n",
        "\n",
        "To move to a real world example, we will start with the Cora dataset. This dataset represents a citation network, where nodes are scientific publications, edges denote citations between them, and features are a [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) extracted from their contents.\n",
        "\n",
        "This graph contains labels for nodes, that represent a specific topic. We will use these for a node classification task.\n",
        "\n",
        "To easily load it, we will use [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/index.html) (PyG), a deep learning library for graph-structured data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd2bTEBADt-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b52f17-df55-4c3b-af3a-0796d48e7170"
      },
      "source": [
        "# Install PyTorch Geometric\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.10.0+cu113.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.12)\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.7/dist-packages (1.5.9)\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.7/dist-packages (1.2.1)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (6.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.1.8)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (0.6.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Nvh_-qEo1q"
      },
      "source": [
        "We can now use the library to download and import the dataset. Initializing the `Planetoid` class returns a `Dataset` object that can contain multiple graphs. In this task we will only use the `Cora` dataset (the citation network) and hence, we will select only the first element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuOvwhsHD2YK"
      },
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "data = Planetoid(root='data/Planetoid', name='Cora')[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from scipy.sparse import coo_matrix\n",
        "from scipy.sparse import diags\n",
        "from scipy.sparse import block_diag\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_scatter import scatter_max\n",
        "\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric.data import DataLoader\n",
        "\n"
      ],
      "metadata": {
        "id": "X6q95JzvScjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4WZkoiHFyZm"
      },
      "source": [
        "\n",
        "#### Question 1 (0.25 pt)\n",
        "\n",
        "The `data` object is an instance of the `Data` class in PyG. Check the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html) and report the following properties of the graph:\n",
        "\n",
        "- Number of nodes\n",
        "- Number of edges \n",
        "- The dimension $c$ of the feature vectors $x\\in\\mathbb{R}^c$\n",
        "- The number of targets for the classification task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjVuGJhlJC_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47210350-5941-4a35-f04b-693d6cc72e55"
      },
      "source": [
        "# Your answer here\n",
        "print(\"nodes: \", data.num_nodes)\n",
        "print(\"edges: \", data.num_edges)\n",
        "print(\"dimension c of features\", data.num_node_features)\n",
        "\n",
        "dataset = Planetoid(root='data/Planetoid', name='Cora')\n",
        "print(\"number of classes: \",dataset.num_classes)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nodes:  2708\n",
            "edges:  10556\n",
            "dimension c of features 1433\n",
            "number of classes:  7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4DrGDAuJ2YO"
      },
      "source": [
        "#### Question 2 (0.25 pt)\n",
        "\n",
        "In PyG, edges are provided in a tensor of shape (2, number of edges). You can access it via `data.edge_index`. Each column in this tensor contains the IDs for two nodes that are connected in the graph.\n",
        "\n",
        "We saw that in an undirected graph, an edge between nodes $i$ and $j$ adds a value of 1 to positions $(i, j)$ and $(j, i)$ of the adjacency matrix. Is this also true for the edge index? That is, if there is an edge $(i, j)$ in `data.edge_index`, is there also an edge for $(j, i)$? This is important to know for the next steps of the implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTRfNxibarRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8d78435-2486-4c86-ea55-47b8b5dbebeb"
      },
      "source": [
        "# Your answer here\n",
        "\n",
        "#check if the graph is undirected \n",
        "print(\"The graph is undirecred: \",data.is_undirected())\n",
        "\n",
        "#check if  there is an edge  (i,j)  in data.edge_index, is there also an edge for  (j,i) ?\n",
        "def check_directed():\n",
        "  row_1 = np.array(data.edge_index[0])\n",
        "  row_2 = np.array(data.edge_index[1])\n",
        "  for i in range(len(row_1)):\n",
        "    check = False\n",
        "    for j in range(len(row_2)):\n",
        "      if row_1[i] == row_2[j] and row_2[i] == row_1[j]:\n",
        "        check = True\n",
        "        break\n",
        "    if not check:\n",
        "      print(\"There is an edge (i,j) in data.edge_index, but edge  (j,i)  in data.edge_index is NOT found\")\n",
        "      return\n",
        "  print(\"There is an edge  (i,j)  in data.edge_index and edge (j,i) is also in data.edge_index\")\n",
        "\n",
        "\n",
        "check_directed()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The graph is undirecred:  True\n",
            "There is an edge  (i,j)  in data.edge_index and edge (j,i) is also in data.edge_index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOpS3QTYiOqp"
      },
      "source": [
        "#### Question 3 (0.5 pt)\n",
        "\n",
        "In graphs, especially large ones, the adjacency matrix is **sparse**: most entries are zero. Sparse matrices allow for efficient storage and computation.\n",
        "\n",
        "To prepare and pre-process sparse matrices, we will use [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html). Once the matrices are ready, we will convert them to PyTorch tensors.\n",
        "\n",
        "We will use the [Sparse COO format](https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO)). We encourage you to first get familiar with how it works after continuing with the assignment.\n",
        "\n",
        "- Use the [`scipy.sparse.coo_matrix()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html) function to build the adjacency matrix. Think of what arguments are needed, and how you can obtain them from the graph data loaded above.\n",
        "- Use the `sum()` method of sparse matrices, together with `scipy.sparse.diags()`, to compute the degree matrix using the definition above.\n",
        "\n",
        "Both resulting matrices must be sparse of type `float32`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC01OjbJs92-"
      },
      "source": [
        "# Your answer here\n",
        "def data_prep(data):\n",
        "  row = np.array(data.edge_index[0])\n",
        "  col = np.array(data.edge_index[1])\n",
        "  d = np.ones(len(data.edge_index[0]), dtype=np.float32)\n",
        "\n",
        "  A = coo_matrix((d, (row, col)), shape=(data.num_nodes, data.num_nodes))\n",
        "  A.toarray()\n",
        "\n",
        "  D = diags(np.array(A.sum(axis=0))[0], 0)\n",
        "\n",
        "  return A, D\n",
        "\n",
        "A, D = data_prep(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIEJyQi2TzyY"
      },
      "source": [
        "You might wonder why we suggest to use a scipy sparse matrix, while also PyTorch supports them. The reason is that in the next step, we will be multiplying two sparse matrices, an operation not supported in PyTorch. PyTorch only allows multiplying a sparse matrix with a dense one, something which we will be doing at a later stage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlmzSb0up4LB"
      },
      "source": [
        "### The Graph Convolutional Network\n",
        "\n",
        "The goal of the graph convolution is to take the feature vectors of all nodes $X\\in\\mathbb{R}^{n\\times c}$, and propagate them along the existing edges, to obtain updated representations $Z\\in\\mathbb{R}^{n\\times d}$.\n",
        "\n",
        "\n",
        "The GCN is initially motivated as performing a convolution, similarly as it is done in CNNs for images, for graph-structured data. In Kipf and Welling (2017), a theoretical derivation leads to the following formula:\n",
        "\n",
        "$$\n",
        "Z = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}XW\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "- $W\\in\\mathbb{R}^{c\\times d}$ is a matrix of parameters to be learned via gradient descent\n",
        "- $\\tilde{A} = A + I_n$, where $I_n$ is an $n\\times n$ identity matrix\n",
        "- $\\tilde{D}$ is the degree matrix computed with $\\tilde{A}$ as the adjacency matrix\n",
        "\n",
        "If we define $\\hat{A} = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$, the graph convolution can be written as $Z = \\hat{A}XW$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL4b-MTvysBp"
      },
      "source": [
        "#### Question 4 (0.25 pt)\n",
        "\n",
        "Given the formula for the GCN, explain why it operates by propagating feature vectors across the graph. To answer this, it might be useful to recall the definitions of the adjacency and degree matrices, and how they are involved in the formula."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgx2SkTTyiSN"
      },
      "source": [
        "# Answer Q4\n",
        "The adjacency matrix is a square matrix of size $n \\times n$ used to represent a graph. The elements of the matrix indicate whether pairs of vertices are adjacent or not in the graph. The matrix is symmetric if the edges are undirected. adding $I_n$, will give a small change, because now the Adjendancy matrix also place's a one at all edges (i,i). This could be interpreted as self connections.\n",
        "\n",
        "The degree matrix of a graph is a diagonal matrix. This matrix contains information about the degree of each vertex that is, the number of edges attached to each vertex $\\tilde{D}$ is equal to $D + I_n$. Taking $\\tilde{D}^{-1/2}$ is an operation, which can be (due to diagonal matrix properties) be performed element wise on every element on the diagonal. $X\\in\\mathbb{R}^{n\\times c}$ is the matrix, which summarices the features (c) of every node (n). \n",
        "\n",
        "Before a graph based regularization was used, which forces nodes that are close together to be labelled the same. This has as downside that it might restrict modelling capacity as the edges does not neccessarily have to encode similarities. Those similarities could also be encode in for example futures. \n",
        "\n",
        "Therefor we use backpropogation of the features, where first all elements in $\\tilde{A}$ are normalized using by $\\frac{1}{\\sqrt{d_i d_j}}$, this leads for a larger decrease for edges of the matrix A, when the nodes have a lot of connections, compared to when the nodes only have a few connections. Next we want to use additional information, which are represented in the features. If we would not backpropogate this information it would not be included in the model, therefor we want to backpropogate the feature values, using this normalization term. So that node x includes some portion of the information of their neighbours. Therefor also the $I_n$ was added, because otherwise the features for the node itself would not be included in the learning.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUGABEqxylsd"
      },
      "source": [
        "#### Question 5 (0.5 pt)\n",
        "\n",
        "Compute the **normalized adjacency matrix** $\\hat{A}$. The result should be a sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPZbnSaSyDzO"
      },
      "source": [
        "# Your answer here\n",
        "def normalized_adjacency(A, D):\n",
        "  A_t = A + diags(np.ones(A.shape[0]) - A.diagonal())\n",
        "  D_t = diags(np.array(A_t.sum(axis=0))[0], 0)\n",
        "\n",
        "  D_t_power = D_t.power(-0.5)\n",
        "  D_t_power.toarray()\n",
        "\n",
        "  A_norm = coo_matrix(A_t.dot(D_t_power).transpose().dot(D_t_power))\n",
        "\n",
        "  return A_norm\n",
        "\n",
        "A_norm = normalized_adjacency(A, D)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLLdGdZoMEy-"
      },
      "source": [
        "#### Question 6 (0.5 pt)\n",
        "\n",
        "So far we have used scipy to build and compute sparse matrices. Since we want to train a GCN with PyTorch, we need to convert $\\hat{A}$ into a sparse PyTorch tensor. You can do this with the [`torch.sparse_coo_tensor()`](https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html) function, making sure to specify `torch.float` as the type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgDsVHzEM32F"
      },
      "source": [
        "# Your answer here\n",
        "def to_torch(A_norm):\n",
        "  values = A_norm.data\n",
        "  indices = np.vstack((A_norm.row, A_norm.col))\n",
        "\n",
        "  i = torch.LongTensor(indices)\n",
        "  v = torch.FloatTensor(values)\n",
        "  shape = A_norm.shape\n",
        "\n",
        "  A_hat = torch.sparse_coo_tensor(i, v, torch.Size(shape), dtype=torch.float32)\n",
        "\n",
        "  return A_hat\n",
        "A_hat = to_torch(A_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_hat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrEsqIk_FrmT",
        "outputId": "832fa5c4-e132-4ad9-d6e6-66a2d2415613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(indices=tensor([[   0,  633, 1862,  ..., 1473, 2706, 2707],\n",
              "                       [   0,    0,    0,  ..., 2707, 2707, 2707]]),\n",
              "       values=tensor([0.2500, 0.2500, 0.2236,  ..., 0.2000, 0.2000, 0.2000]),\n",
              "       size=(2708, 2708), nnz=13264, layout=torch.sparse_coo)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAlRVT5aODkX"
      },
      "source": [
        "#### Question 7 (0.5 pt)\n",
        "\n",
        "We now have all the ingredients to build a GCN layer. Implement a class (inheriting from `torch.nn.Module`) with a learnable matrix of weights $W\\in\\mathbb{R}^{c\\times d}$. Make sure to\n",
        "\n",
        "- Call this class `GCNLayer`\n",
        "- The `__init__()` constructor should take as argument the number of input and output features.\n",
        "- Use `torch.nn.init.kaiming_uniform_` to initialize $W$.\n",
        "- Define the `forward` method, which takes as input $X$ and $\\hat{A}$ and returns $Z$. Note that multiplications involving the sparse matrix $\\hat{A}$ have to be done with `torch.spmm`. \n",
        "\n",
        "Once you have implemented the class, instantiate a layer with the correct number of input features for the Cora dataset, and a number of output features of your choice. Do a forward pass and report the shape of the output tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFCohhhwPpTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44ed6be3-4e28-4b9a-8044-5478c320acc7"
      },
      "source": [
        "# Your answer here\n",
        "class GCNLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  Inits a weight matrix\n",
        "  Inputs:\n",
        "    c_in - Number of features\n",
        "    c_out - Number of channels it wants as output\n",
        "  \"\"\"\n",
        "  def __init__(self, c_in, c_out):\n",
        "      \n",
        "      super(GCNLayer, self).__init__()\n",
        "      self.weight = Parameter(torch.Tensor(c_in, c_out))\n",
        "      torch.nn.init.kaiming_uniform_(self.weight) \n",
        "\n",
        "  def forward(self, X, A_hat):\n",
        "      \"\"\"\n",
        "      Inputs:\n",
        "          X - Tensor of shape [#nodes, c_in] or [#nodes, c_in]\n",
        "          A_hat - Adjacency matrices of the graph. \n",
        "      \"\"\"\n",
        "      Z = torch.matmul(torch.sparse.mm(A_hat, X), self.weight)\n",
        "\n",
        "      return Z\n",
        "\n",
        "# Init class\n",
        "layer = GCNLayer(c_in=data.num_node_features, c_out=dataset.num_classes)\n",
        "\n",
        "# Calculate prediction for Z\n",
        "layer(data.x, A_hat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.4676, -0.4318,  0.1197,  ...,  0.0060,  1.4398, -0.2111],\n",
              "        [ 0.1573, -0.6981,  0.5620,  ...,  1.4054,  0.7447, -0.3761],\n",
              "        [ 0.6501, -2.1099, -1.6343,  ...,  0.9926, -1.3912,  0.3917],\n",
              "        ...,\n",
              "        [ 0.4324, -1.5535, -0.5636,  ..., -2.2149,  0.6109, -0.3303],\n",
              "        [ 1.3482, -2.7836,  0.8372,  ..., -0.3628,  1.1693, -1.5509],\n",
              "        [ 1.5006, -1.8595,  0.5263,  ...,  0.1768,  0.8470, -0.6499]],\n",
              "       grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ptAiizZUKaM"
      },
      "source": [
        "#### Question 8 (0.5 pt)\n",
        "\n",
        "As we have seen so far, the GCN layer implements a special type of linear transformation of the inputs. However, it is often beneficial in deep learning to stack multiple, non-linear transformations of the input features. Implement a second module class for a model with two GCN layers (use the module you implemented in the previous question).\n",
        "\n",
        "- Call this class `GCN`\n",
        "- The constructor must now take as input the number of input features, the output dimension of the first layer (this is the hidden layer), and the output dimension of the output layer.\n",
        "- In the forward pass, add a ReLU activation function after the first layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zhyu3S9Vj3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e676dae0-5cd0-40ac-928e-5b2632b5bc7e"
      },
      "source": [
        "# Your answer here\n",
        "class GCN(nn.Module):\n",
        "  \"\"\"\n",
        "  Inits a weight matrix\n",
        "  Inputs:\n",
        "    c_in - Number of features\n",
        "    c_hidden - Number of channels in hidden layer\n",
        "    c_out - Number of channels it wants as output\n",
        "  \"\"\"\n",
        "  def __init__(self, c_in, c_hidden, c_out):\n",
        "      super(GCN, self).__init__()\n",
        "      self.layer1 = GCNLayer(c_in, c_hidden)\n",
        "      self.layer2 = GCNLayer(c_hidden, c_out)\n",
        "\n",
        "  def forward(self, X, A_hat):\n",
        "    \"\"\"\n",
        "    Performs one GCN pass with relu activation and then one more GCN forward pass\n",
        "    Inputs:\n",
        "        X - Tensor of shape [#nodes, c_in]\n",
        "        A_hat - Adjacency matrices of the graph. \n",
        "    \"\"\"\n",
        "\n",
        "    x = F.relu(self.layer1(X, A_hat))\n",
        "    Z = self.layer2(x, A_hat)\n",
        "\n",
        "    return Z\n",
        "    \n",
        "model_Q8 = GCN(c_in=data.num_node_features,c_hidden=20, c_out=dataset.num_classes)\n",
        "model_Q8(data.x, A_hat).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2708, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NVB-3I5Wfkf"
      },
      "source": [
        "### GCNs for semi-supervised node classification\n",
        "\n",
        "Now that we have a GCN with two layers, we can test its performance in a node classification task. We will pass the input node features $X$ through the GCN layers, and the output will be of size $n\\times k$ where $k$ is the number of classes (which you found in question 1). The label denotes the topic an article in the citation network belongs to (e.g. physics, computer science, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trc4dSa7cuQj"
      },
      "source": [
        "#### Question 9 (1.5 pt)\n",
        "\n",
        "Note that the `data` object contains all labels (for all splits) in `data.y`, and binary masks for the train, validation, and test splits in `data.train_mask`, `data.val_mask`, and `data.test_mask`, respectively. These masks are the same size as `data.y`, and indicate which labels belong to which split.\n",
        "\n",
        "- Create a GCN with two layers (using the class from the previous question), with 32 as the hidden dimension, and the number of output features equal to the number of classes in the Cora dataset.\n",
        "\n",
        "- Use the Adam optimizer with a learning rate of 0.01.\n",
        "\n",
        "- Implement a training loop for the GCN. At each step, pass $X$ and $\\hat{A}$ to the GCN to obtain the logits. Compute the mean cross-entropy loss **only for the training instances**, using the binary masks.\n",
        "\n",
        "- After each training step, evaluate the accuracy for the validation instances.\n",
        "\n",
        "- Train for 100 epochs. Once training is finished, plot the training loss and validation accuracy (in a graph in function of the epoch number), and report the accuracy in the test set.\n",
        "\n",
        "You should obtain an accuracy over 75% on both the validation and test sets. You can also compare your results with the original paper, which also contains results for the Cora dataset. Give a brief discussion on the results of your experiments.\n",
        "\n",
        "Note that in contrast with other tasks, like image classification on some datasets, we don't use mini-batches here. The whole matrix of features and the adjacency is passed to the GCN in one step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z2OP_ZRWlmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7582dfc-c8f7-47c4-8eea-f17378b75996"
      },
      "source": [
        "# Your answer here\n",
        "def forward(data, model, loss_cross, optim, A_hat, mask, mode=\"train\"):\n",
        "  # Compute the model prediction\n",
        "  pred = model(data.x, A_hat)[mask]\n",
        "  #compute Loss & accuracy  \n",
        "  loss = loss_cross(pred, data.y[mask])\n",
        "  winners = pred.argmax(dim=1)\n",
        "  corrects = (winners == data.y[mask])\n",
        "  accuracy = corrects.sum().float() / mask.sum()\n",
        "\n",
        "  # Training step\n",
        "  if mode == \"train\":\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "  return loss, accuracy\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN(c_in=data.num_node_features,c_hidden=32, c_out=dataset.num_classes)\n",
        "model.to(device)\n",
        "data = dataset[0].to(device)\n",
        "A_hat = A_hat.to(device)\n",
        "\n",
        "learning_rate = 0.01\n",
        "batch_size = 1\n",
        "epochs = 100\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=5e-4)\n",
        "model.train()\n",
        "\n",
        "loss_cross = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "for e in range(epochs):\n",
        "  loss, accuracy = forward(data, model, loss_cross, optimizer, A_hat, data.train_mask)\n",
        "  print(\"Epoch number\", e+1, \"Loss:\", loss.item(), \"Acc\", accuracy.item())\n",
        "\n",
        "print(\"---Validation---\")\n",
        "loss, accuracy = forward(data, model, loss_cross, optimizer, A_hat, data.test_mask, mode=\"test\")\n",
        "print(\"Loss:\", loss.item(), \"Acc\", accuracy.item())\n",
        "\n",
        "print(\"---Test---\")\n",
        "loss, accuracy = forward(data, model, loss_cross, optimizer, A_hat, data.val_mask, mode=\"validation\")\n",
        "print(\"Loss:\", loss.item(), \"Acc\", accuracy.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch number 1 Loss: 2.5955398082733154 Acc 0.1071428582072258\n",
            "Epoch number 2 Loss: 1.9912554025650024 Acc 0.18571428954601288\n",
            "Epoch number 3 Loss: 1.6056880950927734 Acc 0.3642857074737549\n",
            "Epoch number 4 Loss: 1.335476279258728 Acc 0.5928571224212646\n",
            "Epoch number 5 Loss: 1.1130262613296509 Acc 0.699999988079071\n",
            "Epoch number 6 Loss: 0.9195192456245422 Acc 0.8071428537368774\n",
            "Epoch number 7 Loss: 0.7525983452796936 Acc 0.8714285492897034\n",
            "Epoch number 8 Loss: 0.6122068166732788 Acc 0.9142857193946838\n",
            "Epoch number 9 Loss: 0.49662983417510986 Acc 0.9357143044471741\n",
            "Epoch number 10 Loss: 0.4024525284767151 Acc 0.9571428298950195\n",
            "Epoch number 11 Loss: 0.3264150023460388 Acc 0.9642857313156128\n",
            "Epoch number 12 Loss: 0.26531684398651123 Acc 0.9785714149475098\n",
            "Epoch number 13 Loss: 0.21611523628234863 Acc 0.9857142567634583\n",
            "Epoch number 14 Loss: 0.17680610716342926 Acc 0.9928571581840515\n",
            "Epoch number 15 Loss: 0.1452232003211975 Acc 0.9928571581840515\n",
            "Epoch number 16 Loss: 0.11918964236974716 Acc 0.9928571581840515\n",
            "Epoch number 17 Loss: 0.09814310818910599 Acc 0.9928571581840515\n",
            "Epoch number 18 Loss: 0.08107484132051468 Acc 0.9928571581840515\n",
            "Epoch number 19 Loss: 0.06713196635246277 Acc 0.9928571581840515\n",
            "Epoch number 20 Loss: 0.055872898548841476 Acc 0.9928571581840515\n",
            "Epoch number 21 Loss: 0.04671858623623848 Acc 0.9928571581840515\n",
            "Epoch number 22 Loss: 0.039307575672864914 Acc 0.9928571581840515\n",
            "Epoch number 23 Loss: 0.03332863003015518 Acc 0.9928571581840515\n",
            "Epoch number 24 Loss: 0.02853940613567829 Acc 1.0\n",
            "Epoch number 25 Loss: 0.024681707844138145 Acc 1.0\n",
            "Epoch number 26 Loss: 0.021566810086369514 Acc 1.0\n",
            "Epoch number 27 Loss: 0.01905072294175625 Acc 1.0\n",
            "Epoch number 28 Loss: 0.017025545239448547 Acc 1.0\n",
            "Epoch number 29 Loss: 0.015390907414257526 Acc 1.0\n",
            "Epoch number 30 Loss: 0.014068244025111198 Acc 1.0\n",
            "Epoch number 31 Loss: 0.012981987558305264 Acc 1.0\n",
            "Epoch number 32 Loss: 0.012068216688930988 Acc 1.0\n",
            "Epoch number 33 Loss: 0.011287904344499111 Acc 1.0\n",
            "Epoch number 34 Loss: 0.010613732971251011 Acc 1.0\n",
            "Epoch number 35 Loss: 0.010027631185948849 Acc 1.0\n",
            "Epoch number 36 Loss: 0.009520472027361393 Acc 1.0\n",
            "Epoch number 37 Loss: 0.009088616818189621 Acc 1.0\n",
            "Epoch number 38 Loss: 0.008725238963961601 Acc 1.0\n",
            "Epoch number 39 Loss: 0.00842734519392252 Acc 1.0\n",
            "Epoch number 40 Loss: 0.008190011605620384 Acc 1.0\n",
            "Epoch number 41 Loss: 0.008009517565369606 Acc 1.0\n",
            "Epoch number 42 Loss: 0.007878638803958893 Acc 1.0\n",
            "Epoch number 43 Loss: 0.007791345473378897 Acc 1.0\n",
            "Epoch number 44 Loss: 0.007742147892713547 Acc 1.0\n",
            "Epoch number 45 Loss: 0.0077260383404791355 Acc 1.0\n",
            "Epoch number 46 Loss: 0.007736903615295887 Acc 1.0\n",
            "Epoch number 47 Loss: 0.00777016207575798 Acc 1.0\n",
            "Epoch number 48 Loss: 0.007822968065738678 Acc 1.0\n",
            "Epoch number 49 Loss: 0.007892335765063763 Acc 1.0\n",
            "Epoch number 50 Loss: 0.007976783439517021 Acc 1.0\n",
            "Epoch number 51 Loss: 0.008077098987996578 Acc 1.0\n",
            "Epoch number 52 Loss: 0.008188544772565365 Acc 1.0\n",
            "Epoch number 53 Loss: 0.008310203440487385 Acc 1.0\n",
            "Epoch number 54 Loss: 0.008441341109573841 Acc 1.0\n",
            "Epoch number 55 Loss: 0.008580822497606277 Acc 1.0\n",
            "Epoch number 56 Loss: 0.00872828159481287 Acc 1.0\n",
            "Epoch number 57 Loss: 0.008881041780114174 Acc 1.0\n",
            "Epoch number 58 Loss: 0.009037652052938938 Acc 1.0\n",
            "Epoch number 59 Loss: 0.009196486324071884 Acc 1.0\n",
            "Epoch number 60 Loss: 0.009357169270515442 Acc 1.0\n",
            "Epoch number 61 Loss: 0.009519096463918686 Acc 1.0\n",
            "Epoch number 62 Loss: 0.009680317714810371 Acc 1.0\n",
            "Epoch number 63 Loss: 0.009839123114943504 Acc 1.0\n",
            "Epoch number 64 Loss: 0.009994389489293098 Acc 1.0\n",
            "Epoch number 65 Loss: 0.010145580396056175 Acc 1.0\n",
            "Epoch number 66 Loss: 0.01029176451265812 Acc 1.0\n",
            "Epoch number 67 Loss: 0.01043117418885231 Acc 1.0\n",
            "Epoch number 68 Loss: 0.010565084405243397 Acc 1.0\n",
            "Epoch number 69 Loss: 0.010691756382584572 Acc 1.0\n",
            "Epoch number 70 Loss: 0.01081043854355812 Acc 1.0\n",
            "Epoch number 71 Loss: 0.010921298526227474 Acc 1.0\n",
            "Epoch number 72 Loss: 0.011022399179637432 Acc 1.0\n",
            "Epoch number 73 Loss: 0.011113641783595085 Acc 1.0\n",
            "Epoch number 74 Loss: 0.011196468956768513 Acc 1.0\n",
            "Epoch number 75 Loss: 0.01127106137573719 Acc 1.0\n",
            "Epoch number 76 Loss: 0.011337199248373508 Acc 1.0\n",
            "Epoch number 77 Loss: 0.011392524465918541 Acc 1.0\n",
            "Epoch number 78 Loss: 0.011436818167567253 Acc 1.0\n",
            "Epoch number 79 Loss: 0.011470982804894447 Acc 1.0\n",
            "Epoch number 80 Loss: 0.01149584911763668 Acc 1.0\n",
            "Epoch number 81 Loss: 0.011512366123497486 Acc 1.0\n",
            "Epoch number 82 Loss: 0.011521909385919571 Acc 1.0\n",
            "Epoch number 83 Loss: 0.011524203233420849 Acc 1.0\n",
            "Epoch number 84 Loss: 0.011519423685967922 Acc 1.0\n",
            "Epoch number 85 Loss: 0.011508316732943058 Acc 1.0\n",
            "Epoch number 86 Loss: 0.011492298915982246 Acc 1.0\n",
            "Epoch number 87 Loss: 0.011471071280539036 Acc 1.0\n",
            "Epoch number 88 Loss: 0.01144598238170147 Acc 1.0\n",
            "Epoch number 89 Loss: 0.011416821740567684 Acc 1.0\n",
            "Epoch number 90 Loss: 0.011383510194718838 Acc 1.0\n",
            "Epoch number 91 Loss: 0.011346060782670975 Acc 1.0\n",
            "Epoch number 92 Loss: 0.011304633691906929 Acc 1.0\n",
            "Epoch number 93 Loss: 0.011260146275162697 Acc 1.0\n",
            "Epoch number 94 Loss: 0.01121231447905302 Acc 1.0\n",
            "Epoch number 95 Loss: 0.01116090826690197 Acc 1.0\n",
            "Epoch number 96 Loss: 0.011106063611805439 Acc 1.0\n",
            "Epoch number 97 Loss: 0.011049283668398857 Acc 1.0\n",
            "Epoch number 98 Loss: 0.01099136658012867 Acc 1.0\n",
            "Epoch number 99 Loss: 0.010934095829725266 Acc 1.0\n",
            "Epoch number 100 Loss: 0.010876862332224846 Acc 1.0\n",
            "---Validation---\n",
            "Loss: 0.6874144673347473 Acc 0.7749999761581421\n",
            "---Test---\n",
            "Loss: 0.7659268379211426 Acc 0.7580000162124634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb5YjHVClCqo"
      },
      "source": [
        "#### Question 10 (0.5 pt)\n",
        "\n",
        "The paper introduces GCNs as a way to solve a *semi-supervised* classification problem.\n",
        "\n",
        "- What makes this problem semi-supervised?\n",
        "- What is the proportion of labeled data used for training with respect to labeled data in the validation and test sets? What is difference in this context with other benchmark tasks in machine learning, like image classification with MNIST?\n",
        "- Why do you think the GCN performs well in this semi-supervised scenario?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q10 ANSWER\n",
        "We speak about semi-supervised learning if the data is only partially labelled. In this specific example \"data.y\" includes all labels, but we don't use all labels. The model is fed with a complete graph of 2708 nodes, then we use only a few labels to make a prediction and back propogate the loss through the complete network \n",
        "\n",
        "We train the model on the complete data set with 2708 nodes, of these 2708 nodes we use only 5% of the data labels, this amount is really small and shows the power of the model as labelling data is a time expensive job. In other supervised benchmark task a lot more labelled data is fed to the machines as it increases understanding, there the goal is to maximize the amount of train data in such a way data the test and validation set are accurate enough to test the and tweek the model performances. But why this difference? A supervised model is trained to do prediction on new data let's say some unseen letteer in MNIST case, but the GCN's are build to predict labels in a network, if you build a network and you already feed it all the labels, why the go through all the trouble?\n",
        "\n",
        "GCN's are especially powerfull in semi-supervised settings, because it understands relation between labelled and unlabelled data. Where image classification only uses an input and weights, the GCNs have next to the input and weights knowledge about how neurons are connected. This leads to a better understanding of also unlabelled data.\n"
      ],
      "metadata": {
        "id": "_kg4qe4u8wg0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ihrjZddvz5d"
      },
      "source": [
        "### Loading a dataset of proteins\n",
        "\n",
        "In the previous sections you learned how to pass the adjacency matrix of a graph with a couple of thousand of nodes, to classify each node with a particular label. A different and useful application of GCNs is graph classification.\n",
        "\n",
        "In contrast with the previous part, where there was a single, big graph, in graph classification we have multiple graphs, and each graph can be assigned a label. In this part of the assignment you will implement a classifier for proteins.\n",
        "\n",
        "[Proteins](https://en.wikipedia.org/wiki/Protein_(nutrient)) are parts of the buildings block of life. They consist of chains of amino acids, and can take many shapes. In the PROTEINS dataset, proteins are represented as graphs, where the nodes are amino acids, and an edge between them indicates that they are 6 [Angstroms](https://en.wikipedia.org/wiki/Angstrom) apart. All graphs have a binary label, where 1 means that the protein is not an enzyme.\n",
        "\n",
        "We will start by loading and examining this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmqweMcvnUH6"
      },
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "dataset = TUDataset(root='data/TU', name='PROTEINS', use_node_attr=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oF1gyKPXiz-"
      },
      "source": [
        "#### Question 11 (0.25 pt)\n",
        "\n",
        "Unlike in the previous part, where we selected the first element returned by the loading function, note that here we get all the elements returned by `TUDataset()`. `dataset` is an interable object, that has some similar behaviors as a Python list: you can call `len()` on it, and you can takes slices from it.\n",
        "\n",
        "Each element in `dataset` is a `Data` object containing a graph that represents a protein. This is the same type of object that we used in the previous part to store the Cora citation network.\n",
        "\n",
        "Knowing this, answer the following:\n",
        "\n",
        "- How many proteins (graphs) are there in `dataset`?\n",
        "- Take any protein from `dataset`. How many nodes and edges does it contain? What is its label? How many features does each node have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNPsnXXbbHHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd8c97a-da1d-4dd5-d10d-86730d71359c"
      },
      "source": [
        "# Your answer here\n",
        "print(\"number of proteins (graphs) in dataset: \", len(dataset))\n",
        "\n",
        "\n",
        "data = dataset[0]\n",
        "\n",
        "print(\"number of nodes of a protein: \", data.num_nodes)\n",
        "print(\"number of edges of a protein: \", data.num_edges)\n",
        "print(\"number of features of each node in a protein: \", data.num_node_features)\n",
        "print(\"labels: \", data.y)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of proteins (graphs) in dataset:  1113\n",
            "number of nodes of a protein:  42\n",
            "number of edges of a protein:  162\n",
            "number of features of each node in a protein:  4\n",
            "labels:  tensor([0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHSklBZXpKpR"
      },
      "source": [
        "#### Question 12 (0.5 pt)\n",
        "\n",
        "To properly train and evaluate our model, we need training, validation, and test splits.\n",
        "\n",
        "For reproducibility purposes, we generate a random tensor of indices for you. Use it to extract the three splits from `dataset`.\n",
        "\n",
        "For training, take 80% of the indices (starting from the first element in `indices`), then the following 10% for validation, and the remaining 10% for testing. You can use the indices to index `dataset`.\n",
        "\n",
        "Call the resulting splits `train_dataset`, `valid_dataset`, and `test_dataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttY4d1GInn08"
      },
      "source": [
        "# Don't erase the following three lines\n",
        "torch.random.manual_seed(0)\n",
        "indices = torch.randperm(len(dataset))\n",
        "\n",
        "# Your answer here\n",
        "train_dataset = []\n",
        "valid_dataset = []\n",
        "test_dataset = []\n",
        "for i in range(len(indices)):\n",
        "  if i <= .8*len(dataset):\n",
        "    train_dataset.append(dataset[indices[i]])\n",
        "  elif i <= .9*len(dataset):\n",
        "    valid_dataset.append(dataset[indices[i]])\n",
        "  else:\n",
        "    test_dataset.append(dataset[indices[i]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDJbB4CQqsfp"
      },
      "source": [
        "### Working with a batch of graphs\n",
        "\n",
        "When working with the Cora dataset, you used the information in `data.edge_index` to build the sparse normalized adjacency matrix $\\hat{A}$ that is required by the GCN. We could do something similar here: for each graph, we build $\\hat{A}$, and pass it to the GCN. However, if the number of graphs is big, this can really slow down training.\n",
        "\n",
        "To avoid this, we will resort to a very useful trick that also allows us to reuse the same GCN you implemented previously. The trick makes it possible to do a forward pass through the GCN for multiple, disconnected graphs at the same time (instead of only one), much like when you train with mini-batches for other kinds of data.\n",
        "\n",
        "Let's first revisit the propagation rule of the GCN, $Z = \\hat{A}XW$, with an illustration (we have omitted the cells of $X$ and $W$ for clarity):\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/02-gcn-forward.png\">\n",
        "\n",
        "If we have multiple graphs, we can still use the same propagation rule, if we\n",
        "\n",
        "- Set $\\hat{A}$ to be a block diagonal matrix, where the blocks are the different adjacency matrices of the graphs\n",
        "- Concatenate the feature matrices along the first dimension\n",
        "\n",
        "This is illustrated in the following figure, for a batch of 3 graphs. Note that the elements outside of the blocks are zero.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/02-gcn-batch-forward.png\">\n",
        "\n",
        "The resulting adjacency matrix $\\hat{A}_B$ can also be built as a sparse matrix, and once we have it together with the concatenated matrix of features, the computation of the graph convolution is exactly the same as before. Note how this trick also allows us to process graphs with different sizes and structures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DLPJ62b2mQ6"
      },
      "source": [
        "#### Question 13 (0.5 pt)\n",
        "\n",
        "\n",
        "Just as the citation network, the graphs in each of the datasets you created in Question 12 also have an `edge_index` attribute, which can be used to compute the normalized adjacency matrix $\\hat{A}$, for each graph.\n",
        "\n",
        "Reusing your code for Questions 3 and 5, define a function `get_a_norm()` that takes as input an element of a dataset (e.g. `train_dataset[0]`), and returns a `scipy.sparse` matrix containing $\\hat{A}$.\n",
        "\n",
        "Note that an element of a dataset has properties like `num_edges`, `num_nodes`, etc. which you can use here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nvPX2GB8oXp"
      },
      "source": [
        "# Your answer here\n",
        "def get_a_norm(data):\n",
        "  row = np.array(data.edge_index[0])\n",
        "  col = np.array(data.edge_index[1])\n",
        "  d = np.ones(len(data.edge_index[0]), dtype=np.float32)\n",
        "\n",
        "  A = coo_matrix((d, (row, col)), shape=(data.num_nodes, data.num_nodes))\n",
        "\n",
        "  np.array(A.sum(axis=0))[0]\n",
        "  D = diags(np.array(A.sum(axis=0))[0], 0)\n",
        "  A_t = A + diags(np.ones(A.shape[0]) - A.diagonal())\n",
        "  D_t = diags(np.array(A_t.sum(axis=0))[0], 0)\n",
        "\n",
        "  D_t_power = D_t.power(-0.5)\n",
        "  D_t_power.toarray()\n",
        "  \n",
        "  A_norm = coo_matrix(A_t.dot(D_t_power).transpose().dot(D_t_power))\n",
        "  return A_norm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBrmYBY3AfhW"
      },
      "source": [
        "#### Question 14 (1 pt)\n",
        "\n",
        "To prepare the batch of graphs, we need to collect multiple adjacency matrices, feature matrices, and labels.\n",
        "\n",
        "When using the trick described in the last figure, we see that we have to keep track of when a graph starts and when it ends, so that we can later differentiate the outputs due to $X^{(0)}$, $X^{(1)}$, etc. To achieve this, we will additionally collect a 1D array of batch indices, one for each $X^{(i)}$.\n",
        "\n",
        "The 1D array has as many elements as rows in $X^{(i)}$, and it is filled with the value $i$ (the position of $X^{(i)}$ in the batch):\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/03-batch-indices.png\">\n",
        "\n",
        "We will later concatenate all the 1D arrays along the first dimension, just as we will do with all the $X^{(i)}$.\n",
        "\n",
        "Define a function `prepare_graphs_batch()` that takes as input a dataset (e.g. `train_dataset`), and does the following\n",
        "\n",
        "- Create four empty lists:\n",
        "  - `adj_matrices`\n",
        "  - `feature_matrices`\n",
        "  - `batch_indices`\n",
        "  - `labels`\n",
        "- Iterate over the input dataset, getting one graph at a time. At each step, use your function from Question 13 to append the adjacency matrix to `adj_matrices`, append the matrix of input features to `feature_matrices`, create the array of batch indices (as explained above) and append it to `batch_indices`, and append the label of the graph to `labels`. **Make sure to convert the label to float**.\n",
        "- Once the loop is over, use `scipy.sparse.block_diag()` to build the block diagonal matrix $\\hat{A}_B$. Convert it to the COO format, and then use your answer to Question 6 to turn it into a sparse PyTorch tensor.\n",
        "- Use `torch.cat()` to concatenate the tensors in `feature_matrices` along the first dimension. Do this also for `batch_indices` and `labels`.\n",
        "- Return the 4 tensors computed in the previous two items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsQ0-JjSqFgD"
      },
      "source": [
        "# Your answer here\n",
        "def prepare_graphs_batch(dataset):\n",
        "  adj_matrices = []\n",
        "  feature_matrices = []\n",
        "  batch_indices = []\n",
        "  labels = []\n",
        "\n",
        "  for idx, i in enumerate(dataset):\n",
        "    adj_matrices.append(get_a_norm(i))\n",
        "    feature_matrices.append(i.x)\n",
        "    batch = torch.from_numpy(np.full((i.num_nodes), idx)) \n",
        "    batch_indices.append(batch)\n",
        "    labels.append(i.y[:])\n",
        "\n",
        "  A_hat_b = coo_matrix(block_diag(adj_matrices))\n",
        "\n",
        "  A_torch = to_torch(A_hat_b)\n",
        "\n",
        "  feature = torch.cat(feature_matrices, 0).requires_grad_(True)\n",
        "\n",
        "  batch = torch.cat(batch_indices, 0)\n",
        "\n",
        "  label = torch.cat(labels,0).to(torch.float)\n",
        "\n",
        "  return A_torch, feature, batch, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i73P_EU0MSPX"
      },
      "source": [
        "Once your answer for the previous question is ready, you can run the next cell to prepare all the required information, for the train, validation, and test splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iol5FxJGMmAU"
      },
      "source": [
        "train_a_norm, train_features, train_batch_idx, train_labels = prepare_graphs_batch(train_dataset)\n",
        "valid_a_norm, valid_features, valid_batch_idx, valid_labels = prepare_graphs_batch(valid_dataset)\n",
        "test_a_norm, test_features, test_batch_idx, test_labels = prepare_graphs_batch(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6q-JU87NClh"
      },
      "source": [
        "### GCNs for graph classification\n",
        "\n",
        "We now have all the ingredients to pass a batch of graphs to a GCN. However, for each graph in the batch, the output $Z^{(i)}$ contains one row for each node in the graph. If the goal is to do classification at the graph level, we have to *pool* these vectors to then compute the required logits for classification.\n",
        "\n",
        "This operation is similar as how pooling works in a CNN. We could consider taking the mean of the vectors, the sum, or use max-pooling. The difference with respect to CNNs is that in our case, we have a batch of graphs, each potentially with a different number of nodes.\n",
        "\n",
        "To implement this specific pooling, we can use the scatter operation in the `torch_scatter` library, which comes when installing PyG. We will use it, together with the tensor of batch indices from the previous two questions, to pool the outputs of the GCN for each graph, into a single vector:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/04-scatter.png\">\n",
        "\n",
        "You can check more details in the [documentation](https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY87DX1uRhnY"
      },
      "source": [
        "#### Question 15 (1.0 pt)\n",
        "\n",
        "Implement a `GraphClassifier` module using PyTorch.\n",
        "\n",
        "- The constructor should take as arguments the number of input features, the hidden dimension, and the number of classes.\n",
        "- The model should contain a instance of the `GCN` module (as you implemented it in Question 8). Use the same value for the hidden dimension and the number of output features (recall that your `GCN` module from Question 8 has two GCN layers).\n",
        "- The model should also contain a `torch.nn.Linear` layer, with the hidden dimension as the input features, and the number of classes as the output.\n",
        "- The forward method receives the concatenated matrix of features, the sparse block diagonal adjacency matrix, and the batch indices (the latter is used when calling `scatter`).\n",
        "- Use the following architecture in the forward pass:\n",
        "  - GCN $\\to$ ReLU $\\to$ scatter (max) $\\to$ Linear.\n",
        "\n",
        "The output of the forward should be a 1D tensor (you might need to call `squeeze` to get rid of extra dimensions) containing the logits for all graphs in the batch, for the binary classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "750WraywwYDH"
      },
      "source": [
        "# Your answer here\n",
        "import scipy\n",
        "\n",
        "class GraphClassifier(nn.Module):\n",
        "  \"\"\"\n",
        "  Inits a weight matrix\n",
        "  Inputs:\n",
        "    c_in - Number of input features\n",
        "    hid - hidden dimensions\n",
        "    c_out - Number of output features\n",
        "  \"\"\"\n",
        "  def __init__(self, c_in, c_hidden, c_out):\n",
        "      super(GraphClassifier, self).__init__()\n",
        "      self.layer1 = GCNLayer(c_in, c_hidden) \n",
        "      self.layer2 = nn.Linear(c_hidden, c_out)\n",
        "      \n",
        "\n",
        "  def forward(self, X, A_hat, batch_indices):\n",
        "      \"\"\"\n",
        "      Inputs:\n",
        "          X - Tensor of shape [#nodes, c_in] or [#nodes, c_in]\n",
        "          A_hat - Adjacency matrices of the graph. \n",
        "          batch_indices - Indices that locates the different batches\n",
        "      \"\"\"\n",
        "      x = self.layer1(X, A_hat).relu()\n",
        "      out, _ = scatter_max(x, batch_indices, dim=0)\n",
        "      layer2_out = self.layer2(out)\n",
        "      output = torch.sigmoid(torch.squeeze(layer2_out))\n",
        "      return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1PHy-_vTjgh"
      },
      "source": [
        "#### Question 16 (1.5 pt)\n",
        "\n",
        "Implement a training loop for the graph classifier. Use the data from Question 14 to train and evaluate the model.\n",
        "\n",
        "We encourage you to use a GPU in this section for faster training. Note that if you change the runtime at this point, you must re-execute several of the cells above, including the ones that install PyG.\n",
        "\n",
        "- Instantiate a classifier with 32 as the hidden dimension\n",
        "- Use Adam with a learning rate of 1e-3.\n",
        "- Use `torch.nn.BCEWithLogitsLoss` as the loss function.\n",
        "- Train for 5,000 epochs. Once training is done, plot the loss curve and the accuracy in the validation set. Then report the accuracy in the test set.\n",
        "\n",
        "**Note:** the logits from the output of the classifier come from a linear layer. To compute actual predictions for the calculation of the accuracy, pass the logits through `torch.sigmoid()`, and set the predicted values to 1 whenever they are greater than 0.5, and to 0 otherwise.\n",
        "\n",
        "You should get an accuracy equal to or higher than 70% in the validation and test sets. Can you beat the [state-of-the-art](https://paperswithcode.com/sota/graph-classification-on-proteins)? Feel free to modify your architecture and experiment with it.\n",
        "\n",
        "Discuss what you observe during training and your results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DbGAs8W2Xja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "935aebf9-e1ac-4645-fb8f-7ed75364e56e"
      },
      "source": [
        "# If your runtime is GPU-enabled, use .to(device) to move the model\n",
        "# and all the relevant tensors to the GPU. You have to move tensors back to CPU\n",
        "# when computing metrics like accuracy, using .cpu().\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "# Your answer here\n",
        "def forward_Graph(features, model, loss_BCE, optim, A_hat, batch_idx, Y, mode=\"train\"):\n",
        "  # Compute the model prediction\n",
        "  pred = model(features, A_hat, batch_idx)\n",
        "\n",
        "  #compute Loss & accuracy \n",
        "  running_loss = loss_BCE(pred, Y)\n",
        "  correct = ((pred > .5).float() == Y)\n",
        "  accuracy = correct.sum() / len(Y)\n",
        "\n",
        "  # Training step\n",
        "  if mode == \"train\":\n",
        "    optim.zero_grad()\n",
        "    running_loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "  return running_loss, accuracy\n",
        "\n",
        "# Check for availability of device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create model\n",
        "model2 = GraphClassifier(c_in=4, c_hidden=32, c_out=1)\n",
        "model2.to(device)\n",
        "model2.train()\n",
        "\n",
        "# Send to the available devide\n",
        "train_a_norm= train_a_norm.to(device) \n",
        "train_features= train_features.to(device)\n",
        "train_batch_idx = train_batch_idx.to(device) \n",
        "train_labels = train_labels.to(device)\n",
        "valid_a_norm = valid_a_norm.to(device) \n",
        "valid_features = valid_features.to(device) \n",
        "valid_batch_idx = valid_batch_idx.to(device)\n",
        "valid_labels = valid_labels.to(device)\n",
        "test_a_norm = test_a_norm.to(device)\n",
        "test_features = test_features.to(device)\n",
        "test_batch_idx = test_batch_idx.to(device)\n",
        "test_labels = test_labels.to(device)\n",
        "\n",
        "# Set hyperparameters\n",
        "learning_rate = 1e-3\n",
        "epochs = 5000\n",
        "\n",
        "# Set optimizer & loss function\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr = learning_rate, weight_decay=5e-4)\n",
        "loss_BCE = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop \n",
        "for e in range(epochs):\n",
        "  running_loss, accuracy = forward_Graph(train_features, model2, loss_BCE, optimizer, train_a_norm, train_batch_idx, train_labels)\n",
        "  if e % 100 == 0:\n",
        "    print(\"Epoch number\", e+1, \"Loss:\", running_loss.item(), \"Acc\", accuracy.item())\n",
        "\n",
        "# Model evaluation\n",
        "print(\"---Validation---\")\n",
        "loss, accuracy = forward_Graph(test_features, model2, loss_BCE, optimizer, test_a_norm, test_batch_idx, test_labels, mode=\"test\")\n",
        "print(\"Loss:\", loss.item(), \"Acc\", accuracy.item())\n",
        "\n",
        "print(\"---Test---\")\n",
        "loss, accuracy = forward_Graph(valid_features, model2, loss_BCE, optimizer, valid_a_norm, valid_batch_idx, valid_labels, mode=\"validation\")\n",
        "print(\"Loss:\", loss.item(), \"Acc\", accuracy.item())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch number 1 Loss: 0.8367689251899719 Acc 0.4074074327945709\n",
            "Epoch number 101 Loss: 0.7044797539710999 Acc 0.5925925970077515\n",
            "Epoch number 201 Loss: 0.693560779094696 Acc 0.5925925970077515\n",
            "Epoch number 301 Loss: 0.6917855739593506 Acc 0.5925925970077515\n",
            "Epoch number 401 Loss: 0.6911154389381409 Acc 0.5925925970077515\n",
            "Epoch number 501 Loss: 0.690687894821167 Acc 0.5925925970077515\n",
            "Epoch number 601 Loss: 0.6903194189071655 Acc 0.5925925970077515\n",
            "Epoch number 701 Loss: 0.689959704875946 Acc 0.5925925970077515\n",
            "Epoch number 801 Loss: 0.689557671546936 Acc 0.5925925970077515\n",
            "Epoch number 901 Loss: 0.6890342235565186 Acc 0.5925925970077515\n",
            "Epoch number 1001 Loss: 0.6882314085960388 Acc 0.5925925970077515\n",
            "Epoch number 1101 Loss: 0.687152087688446 Acc 0.5925925970077515\n",
            "Epoch number 1201 Loss: 0.6858388185501099 Acc 0.5948373079299927\n",
            "Epoch number 1301 Loss: 0.6842437386512756 Acc 0.6127946376800537\n",
            "Epoch number 1401 Loss: 0.6822535991668701 Acc 0.6240180134773254\n",
            "Epoch number 1501 Loss: 0.6801016330718994 Acc 0.6341189742088318\n",
            "Epoch number 1601 Loss: 0.6785594820976257 Acc 0.6453423500061035\n",
            "Epoch number 1701 Loss: 0.6776010990142822 Acc 0.6565656661987305\n",
            "Epoch number 1801 Loss: 0.6769447326660156 Acc 0.6599326729774475\n",
            "Epoch number 1901 Loss: 0.6765127182006836 Acc 0.6599326729774475\n",
            "Epoch number 2001 Loss: 0.6762363314628601 Acc 0.6610550284385681\n",
            "Epoch number 2101 Loss: 0.6760669350624084 Acc 0.6621773838996887\n",
            "Epoch number 2201 Loss: 0.6759525537490845 Acc 0.6632996797561646\n",
            "Epoch number 2301 Loss: 0.6758771538734436 Acc 0.6644220352172852\n",
            "Epoch number 2401 Loss: 0.6758201718330383 Acc 0.6644220352172852\n",
            "Epoch number 2501 Loss: 0.6757700443267822 Acc 0.6632996797561646\n",
            "Epoch number 2601 Loss: 0.675742506980896 Acc 0.6632996797561646\n",
            "Epoch number 2701 Loss: 0.6757177114486694 Acc 0.6632996797561646\n",
            "Epoch number 2801 Loss: 0.675703227519989 Acc 0.6621773838996887\n",
            "Epoch number 2901 Loss: 0.6756941676139832 Acc 0.6621773838996887\n",
            "Epoch number 3001 Loss: 0.6756885051727295 Acc 0.6621773838996887\n",
            "Epoch number 3101 Loss: 0.6756840348243713 Acc 0.6621773838996887\n",
            "Epoch number 3201 Loss: 0.6756802201271057 Acc 0.6621773838996887\n",
            "Epoch number 3301 Loss: 0.6756770014762878 Acc 0.6621773838996887\n",
            "Epoch number 3401 Loss: 0.6756758093833923 Acc 0.6621773838996887\n",
            "Epoch number 3501 Loss: 0.6756736040115356 Acc 0.6621773838996887\n",
            "Epoch number 3601 Loss: 0.6756727695465088 Acc 0.6621773838996887\n",
            "Epoch number 3701 Loss: 0.6756693720817566 Acc 0.6621773838996887\n",
            "Epoch number 3801 Loss: 0.6756682395935059 Acc 0.6621773838996887\n",
            "Epoch number 3901 Loss: 0.6756618618965149 Acc 0.6621773838996887\n",
            "Epoch number 4001 Loss: 0.6756632328033447 Acc 0.6621773838996887\n",
            "Epoch number 4101 Loss: 0.6756588220596313 Acc 0.6621773838996887\n",
            "Epoch number 4201 Loss: 0.6756575703620911 Acc 0.6621773838996887\n",
            "Epoch number 4301 Loss: 0.6756537556648254 Acc 0.6621773838996887\n",
            "Epoch number 4401 Loss: 0.6756515502929688 Acc 0.6621773838996887\n",
            "Epoch number 4501 Loss: 0.6756485104560852 Acc 0.6621773838996887\n",
            "Epoch number 4601 Loss: 0.6756369471549988 Acc 0.6621773838996887\n",
            "Epoch number 4701 Loss: 0.6756375432014465 Acc 0.6621773838996887\n",
            "Epoch number 4801 Loss: 0.6756369471549988 Acc 0.6621773838996887\n",
            "Epoch number 4901 Loss: 0.6756348013877869 Acc 0.6621773838996887\n",
            "---Validation---\n",
            "Loss: 0.6812361478805542 Acc 0.6576576828956604\n",
            "---Test---\n",
            "Loss: 0.6935914754867554 Acc 0.6666666865348816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we conclude with a small summary of the observations we found during training. In the first epoch we expect the accuracy to be close to the .50, because at every single observation we have a 50% chance of guessing correctly. This also holds when running the model. Next we see that we train on the data and we see that the model trains and quite quickly (approx. after 100 epochs) the accuracy stabalizes. Even after 5000 epochs the accuracy appears to be \"stuck\" in a local minimum. We clearly see that the model does not over fit even though it sees exactly the same data at every epoch. This can be found especially well when comparing the train accuracy, with that of the test and validation set."
      ],
      "metadata": {
        "id": "EnosME7rUjri"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvmAfDNMcnKD"
      },
      "source": [
        "## The end\n",
        "\n",
        "If you have made it all the way here successfully, congratulations!  \n",
        "\n",
        "You have implemented your own GCN and tested it on a node classification task, and a more challenging classification task over multiple graphs.\n",
        "\n",
        "We hope you can use this knowledge to apply GCNs not only to the tasks described here, but other applications where data can be modeled as a graph.\n",
        "\n",
        "If you are interested in applying graph neural networks to larger graphs, or try newer architectures, you can dive deeper into [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/), a library with fast implementations for a wide range of architectures. It also comes with custom code that takes care of aspects that you dealt with manually for this assignment, like a more efficient implementation of the adjacency matrix multiplication via message-passing methods, and Data Loaders that relieve you from having to build block diagonal sparse matrices.\n",
        "\n",
        "You can also check the [Deep Graph Library](https://docs.dgl.ai/) another powerful library for deep learning on graphs which also integrates with other backends like TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7QhyAMms8-L"
      },
      "source": [
        "# Grading (10pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juIdxXhos-mV"
      },
      "source": [
        "- Question 1: 0.25pt \n",
        "- Question 2: 0.25pt \n",
        "- Question 3: 0.5pt \n",
        "- Question 4: 0.25pt \n",
        "- Question 5: 0.5pt \n",
        "- Question 6: 0.5pt \n",
        "- Question 7: 0.5pt \n",
        "- Question 8: 0.5pt \n",
        "- Question 9: 1.5pt \n",
        "- Question 10: 0.5pt \n",
        "- Question 11: 0.25pt \n",
        "- Question 12: 0.5pt \n",
        "- Question 13: 0.5pt \n",
        "- Question 14: 1pt\n",
        "- Question 15: 1pt\n",
        "- Question 16: 1.5pt"
      ]
    }
  ]
}